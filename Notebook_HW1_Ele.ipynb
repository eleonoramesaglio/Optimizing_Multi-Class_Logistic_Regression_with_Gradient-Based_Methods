{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization for Data Science 2024 Homework 1\n",
    "\n",
    "Students:\n",
    "\n",
    "Alberto Calabrese Nº:2103405\n",
    "\n",
    "Greta d'Amore Grelli Nº:2122424\n",
    "\n",
    "Eleonora Mesaglio Nº:2103402\n",
    "\n",
    "Marlon Helbing Nº:2106578"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set a seed for deterministic outputs\n",
    "SEED = 0\n",
    "np.random.seed(seed = SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A - MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 1000\n",
    "NUM_FEATURES = 1000\n",
    "\n",
    "# Generate a 1000x1000 matrix with random samples from a standard normal distribution\n",
    "# This is our data matrix, which contains 1000 samples (rows) with 1000 features each (columns)\n",
    "# A MATRIX\n",
    "data_matrix = np.random.normal(0, 1, size = (NUM_SAMPLES, NUM_FEATURES)) # He refers to it as A\n",
    "A = data_matrix \n",
    "# Now 'data_matrix' contains random values drawn from N(0,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X - MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.51424689  1.11286451  0.67502449 ...  0.25882336 -1.19061855\n",
      "   0.33358561]\n",
      " [-1.81915062 -0.47212146 -0.34912377 ...  0.62585093 -0.65290049\n",
      "   0.80570443]\n",
      " [-0.55865306 -0.0035541   0.24477089 ... -0.26494581 -0.44695689\n",
      "  -1.48747178]\n",
      " ...\n",
      " [ 2.33758546 -0.84650435  0.79761719 ...  0.23849016  1.6671718\n",
      "  -0.85339979]\n",
      " [ 1.03501666  0.69079556 -1.11256574 ...  0.08100798 -1.28175412\n",
      "  -1.45574687]\n",
      " [-1.17802047 -0.58835553 -1.21609739 ... -0.39206131 -0.97551536\n",
      "  -1.14216452]]\n"
     ]
    }
   ],
   "source": [
    "NUM_LABELS = 50\n",
    "\n",
    "# This is our weight matrix that we initialize like this ; these weights we want to learn\n",
    "# it has 1000 features (rows) with 50 labels each (columns)\n",
    "# X MATRIX\n",
    "weight_matrix = np.random.normal(0, 1, size = (NUM_FEATURES, NUM_LABELS)) # He refers to it as X \n",
    "X = weight_matrix\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E - MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EXAMPLES = 1000\n",
    "\n",
    "# This matrix is used to help generating our supervised gold labels \n",
    "# It is of size 1000 training examples (rows) and their labels (columns)\n",
    "generative_matrix = np.random.normal(0, 1, size = (NUM_EXAMPLES, NUM_LABELS)) # He refers to it as E \n",
    "E = generative_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AX + E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector with numbers from 1 to 50\n",
    "label_vector = np.arange(1, 51)\n",
    "\n",
    "# Print the vector\n",
    "#print(label_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 28.37955025  12.44004627  74.53765602 ...   5.36347241  29.24640324\n",
      "  -58.68832687]\n",
      " [-36.84008464  -1.55276569 -35.44585854 ...  10.0214222  -13.82201066\n",
      "   -8.29253847]\n",
      " [ 28.64849189  -6.52245633 -31.13462845 ...  73.6132841  -10.96331242\n",
      "  -53.34722266]\n",
      " ...\n",
      " [ 16.19559425  55.91715705  15.65062663 ... -15.05409478  35.96461763\n",
      "   39.89412112]\n",
      " [ 30.58357539   2.84910196 -34.83950691 ...  -3.98929294  -9.25761593\n",
      "   13.50727109]\n",
      " [ -7.68284097  20.49135152  30.88517547 ... -40.13266463  10.54610989\n",
      "  -44.44748546]]\n",
      "(1000, 50)\n"
     ]
    }
   ],
   "source": [
    "# Now he wants us to calculate AX+E to generate labels for the 1000 training examples (such that we have a supervised learning set) +\n",
    "\n",
    "# Calculate the matrix product AX\n",
    "AX = np.matmul(data_matrix, weight_matrix)  # or simply: AX = A @ X\n",
    "print(AX)\n",
    "\n",
    "# Add E to AX element-wise\n",
    "result_matrix = AX + generative_matrix\n",
    "\n",
    "print(result_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MAX INDEX AS CLASS LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2 38 47 33 24 35 22 18 30 49  2 42 23 22  0 29 47 40 30 13 32 23 36  9\n",
      " 29 14  3 38  6 21 37  4 34 39 32 22 12  9 31 27 21 16 24  7 27 39 21 35\n",
      "  8 39 25 27 48 48 46  6 46 37  1 49 27 46 11 25 47 49 24 45 28 27 48 37\n",
      " 21 23  1  3 45 27 18  8 19 11 40 16 10 46  0 19 46 21 10 13 13 13 15 16\n",
      "  7 22  8 14 12 49  8 43 21 17 45 24 17 19 23  3 10  6 10 12 17 29 48 48\n",
      " 22 26  4 27 41 20 46 20 33 30 20 13 44 16 20 35 39  7 16 13 41 48 32 40\n",
      " 44 14 41 37 43 46 48 30 34 38  8 15  9 16  9 23 31 15 27 11 36 25 27  2\n",
      " 12 29 17 14 39 20 47 41 21 14 20 10 23  1 45  2 15 27  5 20 31 23 25 49\n",
      " 33  4 46 31 27 27  6 45 27 30 40 39  9  5 33 32 46 49 41 41 40  7  0 26\n",
      " 19 43 12  0 32 31 13 39 14 22 14 34  8 27  0 26 24 14 25 37 37 32 25 40\n",
      " 11 39 15 25 39 47 17 49 27 25 41 42 25  2 31 49 16 48 38 20 25  4 47 18\n",
      " 12 44 36  7  8 29 41 45 34  4 10 21 11 38 32 41 26 29 15  1 16  6 11 10\n",
      " 17  3 43 36 22 23 11 24 34  0 32 24 18 45 16  3  5 15 25 17 16 20  0 17\n",
      " 36 20 46 46  2 42 45 41 36 28 45 37 37 39 26 21 45 38 45 17 27 44 37 27\n",
      " 12 37 23  6  6 36 42 48 14 34 41  3  3 38 25 32 13  0 33 40 11 23 24 28\n",
      " 20 13 18 42  6  9 39 29  8 23  8 24  8 30 15 26 25  5 40 32 22 39 15 45\n",
      " 21 34 22 11 29 44  3 21 12  4  9 23 32  2 33 24 48 25 23 11 25 15 37 23\n",
      " 41  7  8 49  2  2 45 38 33 22 26  4 26 16 13 40 13 36 44 11 40 35 48  7\n",
      " 34 32 43 46 44  1 24 35 47  3 49  5 29 43 38 40 30  6 49 16 45 43 34  0\n",
      "  4  8  9  9 12 48 17 19 20  2  6  6 25 46 39 41  9 14 25 43 40 30  5 46\n",
      " 29 31  1 42 26 27 36 24 12 11 42 41 38 36 29 45 35  6 24 31  6 22  2 24\n",
      " 11 10 16  9 16 26 31  6  3 22  8  3 20 26 47 46 16 40 37 48 32 27 33  2\n",
      " 26 45 12 48 44  4 13 40 37 39 25 36  2 43 27 12 41 49 34 48  3 46 44  3\n",
      "  3 27 46 48 39 18 44 33 35 28  3  9 27 16 22 37 45 10 17 40 41 11  6 26\n",
      " 22 12 41 47 31  8 38 29 10 42  8 45 28 35  4 21 19 48 25 45  4 16  3 43\n",
      " 40  6 20 21 15 41 37 31 41 18  9 44 32 16 41 35 36 21  0 30 38 37  6 24\n",
      " 16 20 32 35 42  6 37  2 14 49  3  1 11 10  0 48 25 28 20 21 16 41 43  1\n",
      " 38  2 25 30  4  2 37 20 46 16 36 35 47 25 38 33 25 32 38  9  0  2 11 15\n",
      "  5 25  3 17 22 45 30 48 15 33 21 46 31 14 43  1 27 32 28 47 25 14 21 22\n",
      " 37 20 40 10 23 32  0 26 47 11 38 35  8 18 20 45 27 33 11 21 33 28  2 41\n",
      "  1 14  5 37 47 24 41 25 10 21 13  2  6 45 43 46 33  8 14 49 13  8 42 37\n",
      " 39  8 45 39 29  6 10 21 13  2 44 16 10  7 38 28 27 21 13 11 18  4 49  4\n",
      " 10  9 44 16 10 26 32 34 34 37 22 42 28 46 31  3 14 21  8 42 15 46  3 47\n",
      " 30 40  6 13 34 46 24 35 35  8 15 31 21  2 40 26 30 28  0 45 18 17 45  2\n",
      " 28 44 22 27 36 41 40 13 30 41 44 41 11  6 21 14 31 44  3 49  7 36 15 31\n",
      " 43 25  8 25  0  6 39  8 40 27  5  5 41 48  6  2 46 45 22 22 25 13  0 31\n",
      " 10 17  1 10 13 23 11 34 32  3  4 22 30 37 42  9 27 42 33 47 34 39 34 47\n",
      " 24 48 28 32 13 14  1 15 24  0 41 26  6 27  7 18  6 35 25  6 26 10 30 30\n",
      " 36 33 13 39 18 10 40 20 20 23 35 15  0  3 23 36 36 34 36  9  8  5 14 45\n",
      " 30 37  9 46  8 49  2  5 27  5 17 10 26 32  1  4 10 23  9  7 41 32 14 16\n",
      " 44 22 13 30 13 45 39 43 43 33 13 41 38 11 13 22 24 14 40 16  7 37 35 33\n",
      " 27 40 25 40 20 23 25 42 47  8  3 47 13 22 25 45]\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "# We find our labels by considering the max index in the row as the class label\n",
    "\n",
    "# Find the column indices of maximum values for each row\n",
    "labels = np.argmax(result_matrix, axis=1)\n",
    "\n",
    "print(labels)\n",
    "\n",
    "#print(result_matrix[2,:])\n",
    "print(labels.shape)\n",
    "\n",
    "# 'max_indices' now contains the column indices of maximum values for each row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_train = A[0:800,:]\n",
    "A_test = A[800:1000, :]\n",
    "\n",
    "labels_train = labels[0:800]\n",
    "labels_test = labels[800:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the negative log-likelihood function\n",
    "def cost_func(data_matrix, weight_matrix, labels):\n",
    "    scores = np.dot(data_matrix, weight_matrix)\n",
    "    exp_scores = np.exp(scores)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    corect_logprobs = -np.log(probs[range(NUM_EXAMPLES), labels])\n",
    "    data_loss = np.sum(corect_logprobs)\n",
    "    return data_loss\n",
    "\n",
    "\n",
    "# Define the function to compute the gradient of the negative log-likelihood function\n",
    "def gradient(data_matrix, weight_matrix, labels):\n",
    "    scores = np.dot(data_matrix, weight_matrix)\n",
    "    exp_scores = np.exp(scores)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    probs[range(NUM_EXAMPLES), labels] -= 1\n",
    "    dW = np.dot(data_matrix.T, probs)\n",
    "    return dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 4.146766\n",
      "iteration 100: loss 0.207946\n",
      "iteration 200: loss 0.116930\n",
      "iteration 300: loss 0.082825\n",
      "iteration 400: loss 0.064673\n",
      "iteration 500: loss 0.053311\n",
      "iteration 600: loss 0.045492\n",
      "iteration 700: loss 0.039762\n",
      "iteration 800: loss 0.035373\n",
      "iteration 900: loss 0.031897\n"
     ]
    }
   ],
   "source": [
    "# Define the learning rate and the number of iterations\n",
    "learning_rate = 0.01\n",
    "num_iterations = 1000\n",
    "\n",
    "# Perform gradient descent\n",
    "for i in range(num_iterations):\n",
    "    grad = gradient(data_matrix, weight_matrix, max_indices)\n",
    "    weight_matrix -= learning_rate * grad\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(\"iteration %d: loss %f\" % (i, cost_func(data_matrix, weight_matrix, max_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LABELED = 500\n",
    "Y_0 = np.random.rand(NUM_SAMPLES, NUM_LABELS) # define an appropriate starting point\n",
    "assert Y_0.shape == (NUM_SAMPLES, NUM_LABELS)\n",
    "\n",
    "EPSILON = 1e-6 # define small epsilon for stopping criterion\n",
    "MAX_ITER = 2000 # and/or a maximum number of iterations (or even a maximum time)\n",
    "\n",
    "ALPHA = 0.01 # define a fixed stepsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "Y_iterates = [Y_0]\n",
    "times = [0]\n",
    "start = time.time()\n",
    "\n",
    "grad = gradient(data_matrix, weight_matrix, max_indices)\n",
    "while len(Y_iterates) < MAX_ITER and np.linalg.norm(grad) > EPSILON: # TO DO: write the condition for the while loop\n",
    "    new_y = Y_iterates[-1] - ALPHA * grad # write the update\n",
    "    Y_iterates.append(new_y)\n",
    "    times.append(time.time() - start)\n",
    "    # Check the stopping criterion\n",
    "    grad = gradient(data_matrix, weight_matrix, max_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "shape mismatch: indexing arrays could not be broadcast together with shapes (1000,) (1000,50) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(times, [cost_func(data_matrix, weight_matrix, y\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m Y_iterates])\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCPU time (seconds)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObjective function\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(times, [cost_func(data_matrix, weight_matrix, y\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m Y_iterates])\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCPU time (seconds)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObjective function\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m, in \u001b[0;36mcost_func\u001b[0;34m(data_matrix, weight_matrix, labels)\u001b[0m\n\u001b[1;32m      4\u001b[0m exp_scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(scores)\n\u001b[1;32m      5\u001b[0m probs \u001b[38;5;241m=\u001b[39m exp_scores \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(exp_scores, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 6\u001b[0m corect_logprobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mlog(probs[\u001b[38;5;28mrange\u001b[39m(NUM_EXAMPLES), labels])\n\u001b[1;32m      7\u001b[0m data_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(corect_logprobs)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data_loss\n",
      "\u001b[0;31mIndexError\u001b[0m: shape mismatch: indexing arrays could not be broadcast together with shapes (1000,) (1000,50) "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(times, [cost_func(data_matrix, weight_matrix, y.astype(int)) for y in Y_iterates])\n",
    "plt.xlabel('CPU time (seconds)')\n",
    "plt.ylabel('Objective function')\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. BCGD with Randomized Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define as a block a single column in the parameter matrix $X$. Thus, one block defines all features for a single class. As this is a column vector in the matrix $X$, our partial gradient is now only dependent on $c$ (because we have a gradient for all the features of one class).\n",
    "\n",
    "\n",
    "Our partial derivative for one block then looks like the following\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial f(X)}{\\partial X_{c}} = - A^{T} \\cdot (L^{I} - Q) = A^{T} \\cdot (- L^{I} + Q)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "where \n",
    "\n",
    "$A$ has form $m \\times d$ ; it is our given matrix A.\n",
    "\\\n",
    "$L^{I}$ has form $m \\times 1$ ; it is the indicator vector containing $1$'s only at the positions where the label of the current sample $i$ is equal to $c$ and $0$'s everywhere else.\n",
    "$$\n",
    "L_{i}^{I}=\\begin{cases}\n",
    "\t\t\t1 \\quad & \\text{if $label_{i} = c $}\\\\\n",
    "            0 \\quad & \\text{otherwise}\n",
    "\t\t \\end{cases}\n",
    "$$\n",
    "\\\n",
    "$Q$ has form $m \\times 1$ ; it is the vector calculating the exponential expression $\\dfrac{\\exp(x_{c}^{T}a_{i})}{\\sum_{c' = 1}^{k} \\exp(x_{c'}^{T}a_{i}) }$ for each sample $i$\n",
    "\n",
    "Thus, our result will be of form $d \\times 1$\n",
    "\n",
    "Note that the calculations needed for $L^{I}$ and $Q$ only depend on $c$ and $i$ . However, as we know all the samples $i$, we construct the vectors $L^{I}$ and $Q$ which are then only dependent on $c$ .\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### NOT TESTED YET ########\n",
    "m = 1000 # samples\n",
    "d = 1000 # features\n",
    "k = 50   # labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(X,A):\n",
    "    # X_l is m x d, where each row is a column of X and depends on what label the sample has\n",
    "    # for example, the first row of X_l is the column vector of matrix X at the index of the label that sample 1 has\n",
    "    X_l = np.zeros((m,d))\n",
    "    # Iterate over all labels and notice that we have a label for each sample, thus we can use idx directly\n",
    "    for idx,label in enumerate(labels):\n",
    "        X_l[idx,:] = X[:,label]\n",
    "\n",
    "    # Make negative\n",
    "    X_l = -1 * X_l\n",
    "    # Now we have to manually calculate the double sum \n",
    "    final_sum = 0\n",
    "    current = 1 # initial so np.log(1) = 0\n",
    "    for sample_idx in range(m):\n",
    "        final_sum += np.log(current)\n",
    "        current = 0 # so we have the correct start value\n",
    "        for label_idx in range(k):\n",
    "            current += np.exp(np.dot((X[:,label_idx]).T, A[sample_idx,:]))\n",
    "\n",
    "  \n",
    " \n",
    "    return np.sum(np.einsum('ij,ji->i', X_l, A)) + final_sum\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function_full(X,A):\n",
    "    # This function is ugly but just for testing if we did smth wrong\n",
    "    sum_1 = 0\n",
    "    for sample_idx in range(m):\n",
    "        # Take the column at index of the label of the current sample\n",
    "        x_bi = X[:, labels[sample_idx]]\n",
    "        x_bi = -1 * x_bi\n",
    "        a_i = A[sample_idx, :]\n",
    "        sum_1 += (x_bi @ a_i) # automatically calculates (1,1000) x (1000,1)\n",
    "    sum_2 = 0\n",
    "    current = 1 # initial so np.log(1) = 0\n",
    "    for sample_idx in range(m):\n",
    "        sum_2 += np.log(current)\n",
    "        current = 0 # so we have the correct start value\n",
    "        for label_idx in range(k):\n",
    "            current += np.exp((X[:,label_idx]) @ A[sample_idx,:])\n",
    "\n",
    "    return sum_1 + sum_2 \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 1000 # samples\n",
    "d = 1000 # features\n",
    "k = 50   # labels\n",
    "\n",
    "def partial_gradient(X,c):\n",
    "    # We need X as parameter so it changes value across diff gradients\n",
    "\n",
    "    # We define the partial gradient\n",
    "    \n",
    "    # Calculating indicator vector L \n",
    "\n",
    "    # Initialize empty L in size of all samples (=1000)\n",
    "    L = np.zeros((m,1))\n",
    "\n",
    "    # Iterate over labels of each sample\n",
    "    for idx,label in enumerate(labels):\n",
    "        # If there is a label match\n",
    "        if label == c:\n",
    "            # We assign a 1\n",
    "            L[idx] = 1\n",
    "        # If there is no match\n",
    "        else:\n",
    "            # We assign a 0\n",
    "            L[idx] = 0\n",
    "\n",
    "    # Calculating vector Q\n",
    "\n",
    "    # Initialize empty Q in size of all samples (=1000)\n",
    "    \n",
    "    Q = np.zeros((m,1))\n",
    "\n",
    "        \n",
    "    # Iterate over all samples\n",
    "    for curr_sample in range(m):\n",
    "    \n",
    "        nominator = np.exp((X[:,c]) @ (A[curr_sample,:]))\n",
    "    \n",
    "        denominator = 0\n",
    "        # Iterate over all labels for the denominator\n",
    "        for curr_label in range(k):\n",
    "            denominator += np.exp((X[:,curr_label]) @ A[curr_sample,:])\n",
    "    \n",
    "       \n",
    "        Q[curr_sample] = nominator/denominator\n",
    "\n",
    "\n",
    "\n",
    "    return (np.dot(A.T, ((-1 * L) + Q))).squeeze() # returns (1000,)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_gradient(X):\n",
    "    # initialize zero gradient of size (m,)\n",
    "    grad = np.zeros(m)\n",
    "    for label in range(k):\n",
    "        grad = np.column_stack((grad, partial_gradient(X,label)))\n",
    "\n",
    "  \n",
    "\n",
    "    return grad[:,1:] #remove 0 column\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 51)\n",
      "(1000, 50)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss : 23.712323700179695\n",
      "Current Loss : 21.448251156209153\n",
      "Current Loss : 19.706367337174015\n",
      "Current Loss : 19.27881879301276\n",
      "Current Loss : 18.07701073346834\n",
      "Current Loss : 17.510117708385224\n",
      "Current Loss : 16.650187294959323\n",
      "Current Loss : 12.85886372886307\n",
      "Current Loss : 12.06646850980178\n",
      "Current Loss : 11.615038763193297\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[113], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m500\u001b[39m): \u001b[39m# 1000 iterations\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     curr_c \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m,k\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCurrent Loss : \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(cost_function_full(X_0, A)))\n\u001b[1;32m      8\u001b[0m     \u001b[39m# Gradient step\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     X_0[:, curr_c] \u001b[39m=\u001b[39m X_0[:, curr_c] \u001b[39m-\u001b[39m \u001b[39m0.001\u001b[39m \u001b[39m*\u001b[39m partial_gradient(X_0,curr_c) \n",
      "Cell \u001b[0;32mIn[91], line 16\u001b[0m, in \u001b[0;36mcost_function_full\u001b[0;34m(X, A)\u001b[0m\n\u001b[1;32m     14\u001b[0m     current \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39m# so we have the correct start value\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[39mfor\u001b[39;00m label_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(k):\n\u001b[0;32m---> 16\u001b[0m         current \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mexp((X[:,label_idx]) \u001b[39m@\u001b[39;49m A[sample_idx,:])\n\u001b[1;32m     18\u001b[0m \u001b[39mreturn\u001b[39;00m sum_1 \u001b[39m+\u001b[39m sum_2\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "X_0 = X\n",
    "# Calculate Loss in the beginning\n",
    "\n",
    "for i in range(500): # 1000 iterations\n",
    "    curr_c = random.randint(0,k-1)\n",
    "    print(\"Current Loss : {}\".format(cost_function_full(X_0, A)))\n",
    "    # Gradient step\n",
    "\n",
    "    X_0[:, curr_c] = X_0[:, curr_c] - 0.001 * partial_gradient(X_0,curr_c) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss : 23.712323700179695\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCurrent Loss : \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(cost_function_full(X_0, A)))\n\u001b[1;32m      9\u001b[0m \u001b[39m# Gradient step\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[39m# Calculate current gradient\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m gradient_at_i \u001b[39m=\u001b[39m full_gradient(X_0)\n\u001b[1;32m     13\u001b[0m \u001b[39m# Calculate the norm\u001b[39;00m\n\u001b[1;32m     14\u001b[0m gradient_norms\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(gradient_at_i))\n",
      "Cell \u001b[0;32mIn[102], line 5\u001b[0m, in \u001b[0;36mfull_gradient\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m      3\u001b[0m grad \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(m)\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m label \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(k):\n\u001b[0;32m----> 5\u001b[0m     grad \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mcolumn_stack((grad, partial_gradient(X,label)))\n\u001b[1;32m      9\u001b[0m \u001b[39mreturn\u001b[39;00m grad[:,\u001b[39m1\u001b[39m:]\n",
      "Cell \u001b[0;32mIn[103], line 41\u001b[0m, in \u001b[0;36mpartial_gradient\u001b[0;34m(X, c)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[39m# Iterate over all labels for the denominator\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[39mfor\u001b[39;00m curr_label \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(k):\n\u001b[0;32m---> 41\u001b[0m         denominator \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mexp((X[:,curr_label]) \u001b[39m@\u001b[39;49m A[curr_sample,:])\n\u001b[1;32m     44\u001b[0m     Q[curr_sample] \u001b[39m=\u001b[39m nominator\u001b[39m/\u001b[39mdenominator\n\u001b[1;32m     48\u001b[0m \u001b[39mreturn\u001b[39;00m (np\u001b[39m.\u001b[39mdot(A\u001b[39m.\u001b[39mT, ((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39m*\u001b[39m L) \u001b[39m+\u001b[39m Q)))\u001b[39m.\u001b[39msqueeze()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "X_0 = X\n",
    "gradient_norms = []\n",
    "for i in range(25): # 10 iterations\n",
    "    print(\"Current Loss : {}\".format(cost_function_full(X_0, A)))\n",
    "    # Gradient step\n",
    "\n",
    "    # Calculate current gradient\n",
    "    gradient_at_i = full_gradient(X_0)\n",
    "    # Calculate the norm\n",
    "    gradient_norms.append(np.linalg.norm(gradient_at_i))\n",
    "    # Gradient step\n",
    "    X_0 = X_0 - 0.001 * gradient_at_i\n",
    "\n",
    "\n",
    "# Create an array of iteration numbers\n",
    "iterations = np.arange(len(gradient_norms))\n",
    "\n",
    "# Create Gradient Norm Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(iterations, gradient_norms, label='Gradient Norm')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Gradient Norm')\n",
    "plt.title('Gradient Norm across Iterations')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# See how X_0 performs :\n",
    "\n",
    "result_matrix_descent = A @ X_0\n",
    "\n",
    "\n",
    "# Find the column indices of maximum values for each row\n",
    "labels_descent = np.argmax(result_matrix_descent, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "equal_entries = labels == labels_descent\n",
    "\n",
    "# Count the number of True values (equal entries)\n",
    "num_equal_entries = np.sum(equal_entries)\n",
    "\n",
    "# Accuracy :\n",
    "accuracy = num_equal_entries / len(labels)\n",
    "\n",
    "print(accuracy)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
