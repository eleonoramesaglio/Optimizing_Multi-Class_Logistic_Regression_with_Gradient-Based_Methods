{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization for Data Science 2024 Homework 1\n",
    "\n",
    "Students:\n",
    "\n",
    "Alberto Calabrese Nº:2103405\n",
    "\n",
    "Greta d'Amore Grelli Nº:2122424\n",
    "\n",
    "Eleonora Mesaglio Nº:2103402\n",
    "\n",
    "Marlon Helbing Nº:2106578"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set a seed for deterministic outputs\n",
    "SEED = 42\n",
    "np.random.seed(seed = SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A - MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 1000\n",
    "NUM_FEATURES = 1000\n",
    "\n",
    "# Generate a 1000x1000 matrix with random samples from a standard normal distribution\n",
    "# This is our data matrix, which contains 1000 samples (rows) with 1000 features each (columns)\n",
    "# A MATRIX\n",
    "data_matrix = np.random.normal(0, 1, size = (NUM_SAMPLES, NUM_FEATURES)) # He refers to it as A\n",
    "A = data_matrix \n",
    "# Now 'data_matrix' contains random values drawn from N(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X - MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LABELS = 50\n",
    "\n",
    "# This is our weight matrix that we initialize like this ; these weights we want to learn\n",
    "# it has 1000 features (rows) with 50 labels each (columns)\n",
    "# X MATRIX\n",
    "weight_matrix = np.random.normal(0, 1, size = (NUM_FEATURES, NUM_LABELS)) # He refers to it as X \n",
    "X = weight_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E - MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EXAMPLES = 1000\n",
    "\n",
    "# This matrix is used to help generating our supervised gold labels \n",
    "# It is of size 1000 training examples (rows) and their labels (columns)\n",
    "generative_matrix = np.random.normal(0, 1, size = (NUM_EXAMPLES, NUM_LABELS)) # He refers to it as E \n",
    "E = generative_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AX + E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector with numbers from 1 to 50\n",
    "label_vector = np.arange(1, 51)\n",
    "\n",
    "# Print the vector\n",
    "#print(label_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 50)\n"
     ]
    }
   ],
   "source": [
    "# Now he wants us to calculate AX+E to generate labels for the 1000 training examples (such that we have a supervised learning set) +\n",
    "\n",
    "# Calculate the matrix product AX\n",
    "AX = np.matmul(data_matrix, weight_matrix)  # or simply: AX = A @ X\n",
    "\n",
    "# Add E to AX element-wise\n",
    "result_matrix = AX + generative_matrix\n",
    "\n",
    "print(result_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MAX INDEX AS CLASS LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7 43 44  7 49 21 10 15 41 43 33 45  8 31 35 29 28 46 36 36  4 23 14  0\n",
      "  8 29 20 31  8  7 27 47 12 11 33  3  5 16  7 25 10 46 27 41 28  0  1  2\n",
      " 20 48 39 49 36 10 32  4 22 15 11 19 20 30 17 38 49 15 15 12 11  4  5 35\n",
      "  5  4 36  5 34 47 15  0 34  2 35 38 41 21 41 14 29 24  7  0 30  0  9 29\n",
      " 24 12 21 45 49 23  7 26 21  5 36 43 42 30 25 26 39  6 44 21 26 37  0 36\n",
      " 47 22  2 46  5  4 10 10 17  3 43  1 20  9 28 11  9 48 23 35 26 11 27 36\n",
      " 28 27 42 35 35 16 34 42 12  2  7 44 41 40 46 23 33  9  0 42 26 43 29 15\n",
      " 13  7  4  4 33 10  7 32 16 32 30 15 32 48 21  7 12 22 32 19 17 13 26 38\n",
      " 38  3 31 35 19 49 20 39 34 36  4 21 24 21 47 26 39 43 33 28 45 21 13 22\n",
      " 23 15 37 23 30  9 38 44 10  1 27 37 10 16 24  3 29 21  5 25 40 36 26 22\n",
      " 37 12 18 10 10 13  4 39 22 19 33 12 16 44 11 22 21 12  4 45 43 28  9 27\n",
      " 48 40 41 13 15 34 32 36 41 19 43 23  6  9 41 40 18 23 28 41 26 30 15 28\n",
      " 27 36 10 34 16 14  8 49  9 47 21 49 28  9 34 17 45 25 48  6 36 25 38 11\n",
      " 26 31 48 49 27 21 33  9  1  7 43 37 15 15 21 10 36  1  0 11 17 17 26 13\n",
      " 16  8 28 24 31 42 37 37  5 37 13 30 26 40  8 28 37 28 15 39 45 14 22 38\n",
      " 22 11  4 43  3 41 44 29 32 27  3  9  9 18 45 18 22 17 20 41 27 32  7  5\n",
      " 22 33 46  7 47 43  6 13 40 44 22 23  9  2 23 17 10 10 44 22 29 38 12 33\n",
      "  1 23 33 41  3 19 10  6 20 25 15 30 36 15 33  0 12  7 31 35  2  4  2  3\n",
      " 34 42 46 22 10 30 25 28 18  2 42 33 21 33  5 16  0  5 15 12 44 46 26 34\n",
      " 18 34 10 17 30  7 44 24 33 47 35  3 22  4 28 14 33 16  9 41 30 20 18 44\n",
      " 29 43  5 28 19 13 18  8 20 31  3 21 13 45  0 21  5  8  6 28 25 38 24 18\n",
      " 46 18 43 20 21 33  2 23 14 14 33  8 37 43  2  2  8 34 29  1 24 20  6  5\n",
      " 34 10 44 13  1  3 21 44 16 15 34 47 49 12 30 36  3 21  7  3 46 25 14  7\n",
      " 24 40 38 25 22  8 34  7 12 48 36 38  8 12 28 33 42 19  0  6 20 10 34 13\n",
      " 24  6 17 42 31 12  9 12 17  6 45  7  6 47  9 15 32 37  4 32 21 18 10 41\n",
      " 31 49  2 16 29 46 12 24  2  7 48 34 15 22 26 12 39 49 25 30 35 18 36  9\n",
      " 28 43 19 45 23 25 34 20  0 20 38 37 36 36  3 30  3 33 37 14 34  8 31 29\n",
      " 17 28  3 19 14  7 33 26 32 43 41 14 30 34 18 14  4 18 38 15  3 48 11 38\n",
      " 39 36 33 36  8 47 43 30 48 30  6  9 22 14  8 43  3  9 19  4  2 40 46 31\n",
      "  4 49 26 48 44 43 35 23 19 34 39 19 29 40 27 18 31 45 43 15 42 23 47 15\n",
      " 32 46 32 22 17 43 19 30  9 28 16 12 44 31 47 23 16 24 18  0 34 38 29  4\n",
      "  1 34 47  2 20 43 29 36 39 15 16 48 47 22 46 28 46 46 37  7 26 33 33 11\n",
      "  4 22 18 10 22  5 34 49 11 27 45 48 12 33 28 19 15 17 41 33 25 46 13 27\n",
      "  9  1 26 33 43  8 32 26  0 36  0 28  5 10  4 48 16 39 43 36 27  6 26 42\n",
      " 12 37 41 24 33  9 15 29 36 24  2 44  2 20 24 15 21 21  2  4 36 32 29  2\n",
      "  0 21 32 31 46  9 23 30 35 17 43 23  9  7 35  5 29 36 25 46 33 14 30 17\n",
      " 14 48 28 23 38 10 41 16 13 18 23 30  0 16 29 14 20  1  7 29 31  2 35 17\n",
      " 47 18 26 43  0 21 14 16 11 15 23  5 27 14  0  1 33 31 33  7  2 37  1 38\n",
      "  4 40  5 27 17 36 32 37  5  6 48 49 28 14 37  6 47  7 21 22 11  5 25 13\n",
      " 38 13 32 16 28 34 25  2 37 18 26 18 14  8 20 22 29 34 23 41 38 22 37 26\n",
      " 38  1  6 34 38 47 10 13 42  7  0 11 37 49 19 37  8 35 23 47 45 19 22 16\n",
      " 42 44 16 42  0 34  4  9 44  1 40 18 40 10 45 49]\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "# We find our labels by considering the max index in the row as the class label\n",
    "\n",
    "# Find the column indices of maximum values for each row\n",
    "labels = np.argmax(result_matrix, axis=1)\n",
    "\n",
    "print(labels)\n",
    "\n",
    "#print(result_matrix[2,:])\n",
    "print(labels.shape)\n",
    "\n",
    "# 'max_indices' now contains the column indices of maximum values for each row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the negative log-likelihood function\n",
    "def cost_func(data_matrix, weight_matrix, labels):\n",
    "    scores = np.dot(data_matrix, weight_matrix)\n",
    "    exp_scores = np.exp(scores)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    corect_logprobs = -np.log(probs[range(NUM_EXAMPLES), labels])\n",
    "    data_loss = np.sum(corect_logprobs)\n",
    "    return data_loss\n",
    "\n",
    "\n",
    "# Define the function to compute the gradient of the negative log-likelihood function\n",
    "def gradient(data_matrix, weight_matrix, labels):\n",
    "    scores = np.dot(data_matrix, weight_matrix)\n",
    "    exp_scores = np.exp(scores)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    probs[range(NUM_EXAMPLES), labels] -= 1\n",
    "    dW = np.dot(data_matrix.T, probs)\n",
    "    return dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 4.146766\n",
      "iteration 100: loss 0.207946\n",
      "iteration 200: loss 0.116930\n",
      "iteration 300: loss 0.082825\n",
      "iteration 400: loss 0.064673\n",
      "iteration 500: loss 0.053311\n",
      "iteration 600: loss 0.045492\n",
      "iteration 700: loss 0.039762\n",
      "iteration 800: loss 0.035373\n",
      "iteration 900: loss 0.031897\n"
     ]
    }
   ],
   "source": [
    "# Define the learning rate and the number of iterations\n",
    "learning_rate = 0.01\n",
    "num_iterations = 1000\n",
    "\n",
    "# Perform gradient descent\n",
    "for i in range(num_iterations):\n",
    "    grad = gradient(data_matrix, weight_matrix, max_indices)\n",
    "    weight_matrix -= learning_rate * grad\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(\"iteration %d: loss %f\" % (i, cost_func(data_matrix, weight_matrix, max_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LABELED = 500\n",
    "Y_0 = np.random.rand(NUM_SAMPLES, NUM_LABELS) # define an appropriate starting point\n",
    "assert Y_0.shape == (NUM_SAMPLES, NUM_LABELS)\n",
    "\n",
    "EPSILON = 1e-6 # define small epsilon for stopping criterion\n",
    "MAX_ITER = 2000 # and/or a maximum number of iterations (or even a maximum time)\n",
    "\n",
    "ALPHA = 0.01 # define a fixed stepsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "Y_iterates = [Y_0]\n",
    "times = [0]\n",
    "start = time.time()\n",
    "\n",
    "grad = gradient(data_matrix, weight_matrix, max_indices)\n",
    "while len(Y_iterates) < MAX_ITER and np.linalg.norm(grad) > EPSILON: # TO DO: write the condition for the while loop\n",
    "    new_y = Y_iterates[-1] - ALPHA * grad # write the update\n",
    "    Y_iterates.append(new_y)\n",
    "    times.append(time.time() - start)\n",
    "    # Check the stopping criterion\n",
    "    grad = gradient(data_matrix, weight_matrix, max_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "shape mismatch: indexing arrays could not be broadcast together with shapes (1000,) (1000,50) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(times, [cost_func(data_matrix, weight_matrix, y\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m Y_iterates])\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCPU time (seconds)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObjective function\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(times, [cost_func(data_matrix, weight_matrix, y\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m Y_iterates])\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCPU time (seconds)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObjective function\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m, in \u001b[0;36mcost_func\u001b[0;34m(data_matrix, weight_matrix, labels)\u001b[0m\n\u001b[1;32m      4\u001b[0m exp_scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(scores)\n\u001b[1;32m      5\u001b[0m probs \u001b[38;5;241m=\u001b[39m exp_scores \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(exp_scores, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 6\u001b[0m corect_logprobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mlog(probs[\u001b[38;5;28mrange\u001b[39m(NUM_EXAMPLES), labels])\n\u001b[1;32m      7\u001b[0m data_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(corect_logprobs)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data_loss\n",
      "\u001b[0;31mIndexError\u001b[0m: shape mismatch: indexing arrays could not be broadcast together with shapes (1000,) (1000,50) "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(times, [cost_func(data_matrix, weight_matrix, y.astype(int)) for y in Y_iterates])\n",
    "plt.xlabel('CPU time (seconds)')\n",
    "plt.ylabel('Objective function')\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. BCGD with Randomized Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define as a block a single column in the parameter matrix $X$. Thus, one block defines all features for a single class. As this is a column vector in the matrix $X$, our partial gradient is now only dependent on $c$ (because we have a gradient for all the features of one class).\n",
    "\n",
    "\n",
    "Our partial derivative for one block then looks like the following\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial f(X)}{\\partial X_{c}} = - A^{T} \\cdot (L^{I} - Q) = A^{T} \\cdot (- L^{I} + Q)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "where \n",
    "\n",
    "$A$ has form $m \\times d$ ; it is our given matrix A.\n",
    "\\\n",
    "$L^{I}$ has form $m \\times 1$ ; it is the indicator vector containing $1$'s only at the positions where the label of the current sample $i$ is equal to $c$ and $0$'s everywhere else.\n",
    "$$\n",
    "L_{i}^{I}=\\begin{cases}\n",
    "\t\t\t1 \\quad & \\text{if $label_{i} = c $}\\\\\n",
    "            0 \\quad & \\text{otherwise}\n",
    "\t\t \\end{cases}\n",
    "$$\n",
    "\\\n",
    "$Q$ has form $m \\times 1$ ; it is the vector calculating the exponential expression $\\dfrac{\\exp(x_{c}^{T}a_{i})}{\\sum_{c' = 1}^{k} \\exp(x_{c'}^{T}a_{i}) }$ for each sample $i$\n",
    "\n",
    "Thus, our result will be of form $d \\times 1$\n",
    "\n",
    "Note that the calculations needed for $L^{I}$ and $Q$ only depend on $c$ and $i$ . However, as we know all the samples $i$, we construct the vectors $L^{I}$ and $Q$ which are then only dependent on $c$ .\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### NOT TESTED YET ########\n",
    "m = 1000 # samples\n",
    "d = 1000 # features\n",
    "k = 50   # labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(X,A):\n",
    "    # X_l is m x d, where each row is a column of X and depends on what label the sample has\n",
    "    # for example, the first row of X_l is the column vector of matrix X at the index of the label that sample 1 has\n",
    "    X_l = np.zeros((m,d))\n",
    "    # Iterate over all labels and notice that we have a label for each sample, thus we can use idx directly\n",
    "    for idx,label in enumerate(labels):\n",
    "        X_l[idx,:] = X[:,label]\n",
    "\n",
    "    # Make negative\n",
    "    X_l = -1 * X_l\n",
    "    # Now we have to manually calculate the double sum \n",
    "    final_sum = 0\n",
    "    current = 1 # initial so np.log(1) = 0\n",
    "    for sample_idx in range(m):\n",
    "        final_sum += np.log(current)\n",
    "        current = 0 # so we have the correct start value\n",
    "        for label_idx in range(k):\n",
    "            current += np.exp(np.dot((X[:,label_idx]).T, A[sample_idx,:]))\n",
    "\n",
    "  \n",
    " \n",
    "    return np.sum(np.einsum('ij,ji->i', X_l, A)) + final_sum\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### NOT TESTED YET ########\n",
    "m = 1000 # samples\n",
    "d = 1000 # features\n",
    "k = 50   # labels\n",
    "\n",
    "def partial_gradient(c):\n",
    "\n",
    "\n",
    "    # We define the partial gradient\n",
    "    \n",
    "    # Calculating indicator vector L \n",
    "\n",
    "    # Initialize empty L in size of all samples (=1000)\n",
    "    L = np.zeros((m,1))\n",
    "\n",
    "    # Iterate over labels of each sample\n",
    "    for idx,label in enumerate(labels):\n",
    "        # If there is a label match\n",
    "        if label == c:\n",
    "            # We assign a 1\n",
    "            L[idx] = 1\n",
    "        # If there is no match\n",
    "        else:\n",
    "            # We assign a 0\n",
    "            L[idx] = 0\n",
    "\n",
    "    # Calculating vector Q\n",
    "\n",
    "    # Initialize empty Q in size of all samples (=1000)\n",
    "    \n",
    "    Q = np.zeros((m,1))\n",
    "\n",
    "        \n",
    "    # Iterate over all samples\n",
    "    for curr_sample in range(m):\n",
    "    \n",
    "        nominator = (np.exp(np.dot(np.expand_dims((X[:,c]),axis=1).T , np.expand_dims(A[curr_sample,:], axis=1)))).item()\n",
    "    \n",
    "        denominator = 0\n",
    "        # Iterate over all labels for the denominator\n",
    "        for curr_label in range(k):\n",
    "            denominator += (np.exp(np.dot(np.expand_dims((X[:,curr_label]), axis=1).T , np.expand_dims(A[curr_sample,:], axis=1)))).item()\n",
    "    \n",
    "       \n",
    "        Q[curr_sample] = nominator/denominator\n",
    "\n",
    "\n",
    "\n",
    "    return np.squeeze(np.dot(A.T ,((-1 * L) + Q)),axis=1)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss : 84664.755369136\n",
      "Current Loss : 84653.119658466\n",
      "Current Loss : 84665.70530135556\n",
      "Current Loss : 84595.38374802489\n",
      "Current Loss : 84557.33266659845\n",
      "Current Loss : 84555.66121399049\n",
      "Current Loss : 84556.38745000251\n",
      "Current Loss : 84574.1124155602\n",
      "Current Loss : 84547.27410254144\n",
      "Current Loss : 84518.37053526223\n",
      "Current Loss : 84490.28949723902\n",
      "Current Loss : 84475.86830469407\n",
      "Current Loss : 84471.84634248387\n",
      "Current Loss : 84460.60242799712\n",
      "Current Loss : 84461.03659703243\n",
      "Current Loss : 84461.23327309552\n",
      "Current Loss : 84492.24855654861\n",
      "Current Loss : 84498.69929787453\n",
      "Current Loss : 84506.35567411034\n",
      "Current Loss : 84517.46056045547\n",
      "Current Loss : 84517.7089128008\n",
      "Current Loss : 84507.95531481603\n",
      "Current Loss : 84496.97055248452\n",
      "Current Loss : 84494.39173001357\n",
      "Current Loss : 84496.60436004536\n",
      "Current Loss : 84441.7232868983\n",
      "Current Loss : 84446.2668658384\n",
      "Current Loss : 84438.75856684291\n",
      "Current Loss : 84398.51108572946\n",
      "Current Loss : 84399.73538594737\n",
      "Current Loss : 84384.35462422796\n",
      "Current Loss : 84380.47303980196\n",
      "Current Loss : 84381.37554272877\n",
      "Current Loss : 84282.09118195708\n",
      "Current Loss : 84043.68830009553\n",
      "Current Loss : 84048.27907733842\n",
      "Current Loss : 83933.88297591239\n",
      "Current Loss : 83924.44337361376\n",
      "Current Loss : 83923.92468787922\n",
      "Current Loss : 83913.97202838464\n",
      "Current Loss : 83917.91159511714\n",
      "Current Loss : 83918.09073165324\n",
      "Current Loss : 83918.16402289356\n",
      "Current Loss : 83928.36792638834\n",
      "Current Loss : 83908.76979278671\n",
      "Current Loss : 83844.13893613662\n",
      "Current Loss : 83844.65008320662\n",
      "Current Loss : 83745.95722908108\n",
      "Current Loss : 83744.94909682381\n",
      "Current Loss : 83746.58230528834\n",
      "Current Loss : 83687.62872669785\n",
      "Current Loss : 83669.29373483256\n",
      "Current Loss : 83670.92291025126\n",
      "Current Loss : 83674.3804753752\n",
      "Current Loss : 83674.32140777935\n",
      "Current Loss : 83675.68888903916\n",
      "Current Loss : 83490.92720720683\n",
      "Current Loss : 83491.2066454853\n",
      "Current Loss : 83491.10297567864\n",
      "Current Loss : 83478.0702239097\n",
      "Current Loss : 83477.52801354919\n",
      "Current Loss : 83483.05767405369\n",
      "Current Loss : 83482.35582568246\n",
      "Current Loss : 83482.67207433577\n",
      "Current Loss : 83483.6989627362\n",
      "Current Loss : 83484.77258763935\n",
      "Current Loss : 83508.10039871326\n",
      "Current Loss : 83516.55696487837\n",
      "Current Loss : 83502.12053534672\n",
      "Current Loss : 83500.4147574445\n",
      "Current Loss : 83510.92179406741\n",
      "Current Loss : 83482.53936180587\n",
      "Current Loss : 83483.68873700168\n",
      "Current Loss : 83482.33999845678\n",
      "Current Loss : 83484.01058799746\n",
      "Current Loss : 83484.0546096026\n",
      "Current Loss : 83482.92809488697\n",
      "Current Loss : 83481.05186924657\n",
      "Current Loss : 83480.74275580114\n",
      "Current Loss : 83391.95427079755\n",
      "Current Loss : 83394.10317597429\n",
      "Current Loss : 83390.82082677302\n",
      "Current Loss : 83391.89397635721\n",
      "Current Loss : 83390.79345953865\n",
      "Current Loss : 83390.53262025848\n",
      "Current Loss : 83383.42234880093\n",
      "Current Loss : 83383.57841049813\n",
      "Current Loss : 83384.3917908677\n",
      "Current Loss : 83385.1922703794\n",
      "Current Loss : 83376.54020925189\n",
      "Current Loss : 83375.10408586886\n",
      "Current Loss : 83299.68236708562\n",
      "Current Loss : 83299.70285745917\n",
      "Current Loss : 83301.03066373254\n",
      "Current Loss : 83294.10504950109\n",
      "Current Loss : 83292.05161128208\n",
      "Current Loss : 83293.02400025075\n",
      "Current Loss : 83263.2221271744\n",
      "Current Loss : 83260.85569580716\n",
      "Current Loss : 83261.73860516874\n",
      "Current Loss : 83264.6556241761\n",
      "Current Loss : 83267.79860264389\n",
      "Current Loss : 83262.1614289784\n",
      "Current Loss : 83261.13796253373\n",
      "Current Loss : 83250.3443573405\n",
      "Current Loss : 83243.60728460993\n",
      "Current Loss : 83247.6801344858\n",
      "Current Loss : 83246.80760584804\n",
      "Current Loss : 83174.72219952475\n",
      "Current Loss : 83178.59033672248\n",
      "Current Loss : 83179.5022140789\n",
      "Current Loss : 83180.17466291976\n",
      "Current Loss : 83020.34181750218\n",
      "Current Loss : 83020.44467643398\n",
      "Current Loss : 83012.83411748253\n",
      "Current Loss : 83008.05640069957\n",
      "Current Loss : 83009.86955743047\n",
      "Current Loss : 83009.97217046702\n",
      "Current Loss : 83010.53297854027\n",
      "Current Loss : 83017.65992113772\n",
      "Current Loss : 83011.57564662726\n",
      "Current Loss : 83012.47960126017\n",
      "Current Loss : 82962.59084499098\n",
      "Current Loss : 82976.57626948059\n",
      "Current Loss : 82981.7764506476\n",
      "Current Loss : 82970.74452671182\n",
      "Current Loss : 82970.8543822254\n",
      "Current Loss : 82964.53780483737\n",
      "Current Loss : 82960.14439593669\n",
      "Current Loss : 82965.9798205893\n",
      "Current Loss : 82960.7663304649\n",
      "Current Loss : 82960.13910950765\n",
      "Current Loss : 82955.8483710456\n",
      "Current Loss : 82954.25170030477\n",
      "Current Loss : 82951.72907639948\n",
      "Current Loss : 82951.17671978236\n",
      "Current Loss : 82936.04523883696\n",
      "Current Loss : 82914.97843282111\n",
      "Current Loss : 82910.73690662523\n",
      "Current Loss : 82903.84921266571\n",
      "Current Loss : 82903.72864337654\n",
      "Current Loss : 82904.19159641505\n",
      "Current Loss : 82910.069184681\n",
      "Current Loss : 82910.7882657778\n",
      "Current Loss : 82905.15238849296\n",
      "Current Loss : 82897.20120111792\n",
      "Current Loss : 82849.88666039002\n",
      "Current Loss : 82848.53120049865\n",
      "Current Loss : 82847.68363959067\n",
      "Current Loss : 82847.9731026299\n",
      "Current Loss : 82847.75300863931\n",
      "Current Loss : 82850.90152355835\n",
      "Current Loss : 82849.90359870183\n",
      "Current Loss : 82852.85716047962\n",
      "Current Loss : 82852.7577557003\n",
      "Current Loss : 82802.24328101831\n",
      "Current Loss : 82793.87977159588\n",
      "Current Loss : 82783.69126478821\n",
      "Current Loss : 82774.36730999625\n",
      "Current Loss : 82771.54352650301\n",
      "Current Loss : 82718.97196414774\n",
      "Current Loss : 82700.75975814661\n",
      "Current Loss : 82698.56906232992\n",
      "Current Loss : 82699.71955422175\n",
      "Current Loss : 82628.17342455949\n",
      "Current Loss : 82628.1815674501\n",
      "Current Loss : 82627.64392214165\n",
      "Current Loss : 82626.95083575234\n",
      "Current Loss : 82625.34156249413\n",
      "Current Loss : 82594.90453251929\n",
      "Current Loss : 82596.37059607924\n",
      "Current Loss : 82546.07409218518\n",
      "Current Loss : 82531.91142356207\n",
      "Current Loss : 82530.49044006313\n",
      "Current Loss : 82536.2942682318\n",
      "Current Loss : 82528.38155939696\n",
      "Current Loss : 82529.54074947251\n",
      "Current Loss : 82529.94706150159\n",
      "Current Loss : 82519.02023064457\n",
      "Current Loss : 82515.92086874516\n",
      "Current Loss : 82515.86138156593\n",
      "Current Loss : 82512.40587754284\n",
      "Current Loss : 82511.34457879665\n",
      "Current Loss : 82509.12246138258\n",
      "Current Loss : 82509.4579790446\n",
      "Current Loss : 82505.96893718491\n",
      "Current Loss : 82498.43733383821\n",
      "Current Loss : 82498.70123999083\n",
      "Current Loss : 82493.18536102182\n",
      "Current Loss : 82494.14048074007\n",
      "Current Loss : 82493.8232411592\n",
      "Current Loss : 82493.88044675511\n",
      "Current Loss : 82501.76422614111\n",
      "Current Loss : 82502.23448683678\n",
      "Current Loss : 82507.1353929205\n",
      "Current Loss : 82505.22499980617\n",
      "Current Loss : 82503.54232060979\n",
      "Current Loss : 82496.03074758318\n",
      "Current Loss : 82494.00875392414\n",
      "Current Loss : 82478.23431224994\n",
      "Current Loss : 82470.46181277395\n",
      "Current Loss : 82471.08967314189\n",
      "Current Loss : 82471.72333581887\n",
      "Current Loss : 82471.82662440099\n",
      "Current Loss : 82468.29789839592\n",
      "Current Loss : 82468.61370546286\n",
      "Current Loss : 82469.31498648986\n",
      "Current Loss : 82469.28137904765\n",
      "Current Loss : 82470.59235555565\n",
      "Current Loss : 82468.0439224775\n",
      "Current Loss : 82460.35260997989\n",
      "Current Loss : 82460.57190190461\n",
      "Current Loss : 82463.48907675949\n",
      "Current Loss : 82461.58172782311\n",
      "Current Loss : 82433.7966263979\n",
      "Current Loss : 82433.32671824307\n",
      "Current Loss : 82431.88705667628\n",
      "Current Loss : 82430.44591679316\n",
      "Current Loss : 82429.55216428472\n",
      "Current Loss : 82429.6533383158\n",
      "Current Loss : 82428.73808291035\n",
      "Current Loss : 82428.86809289112\n",
      "Current Loss : 82393.67679017696\n",
      "Current Loss : 82337.3381416206\n",
      "Current Loss : 82336.39480335766\n",
      "Current Loss : 82274.80661717741\n",
      "Current Loss : 82264.80996402509\n",
      "Current Loss : 82264.55431322109\n",
      "Current Loss : 82223.59748782373\n",
      "Current Loss : 82223.41805454118\n",
      "Current Loss : 82223.66496136009\n",
      "Current Loss : 82223.4429682691\n",
      "Current Loss : 82221.91826822842\n",
      "Current Loss : 82221.01714264158\n",
      "Current Loss : 82217.02641155351\n",
      "Current Loss : 82217.0917171172\n",
      "Current Loss : 82217.0563328527\n",
      "Current Loss : 82218.2277336357\n",
      "Current Loss : 82221.54442495528\n",
      "Current Loss : 82220.61348021342\n",
      "Current Loss : 82183.72562707898\n",
      "Current Loss : 82183.15243545803\n",
      "Current Loss : 82180.25651850321\n",
      "Current Loss : 82180.28229752372\n",
      "Current Loss : 82181.07032234083\n",
      "Current Loss : 82181.59011304488\n",
      "Current Loss : 82182.14932465079\n",
      "Current Loss : 82182.28323965038\n",
      "Current Loss : 82147.13455227249\n",
      "Current Loss : 82147.26706467223\n",
      "Current Loss : 82148.34897876515\n",
      "Current Loss : 82153.57069871496\n",
      "Current Loss : 82154.01137078219\n",
      "Current Loss : 82154.4697527532\n",
      "Current Loss : 82157.78157051982\n",
      "Current Loss : 82126.43498761716\n",
      "Current Loss : 82121.10066671409\n",
      "Current Loss : 82121.72229286007\n",
      "Current Loss : 82118.24376586192\n",
      "Current Loss : 82113.90262330305\n",
      "Current Loss : 82113.92224510005\n",
      "Current Loss : 82114.79536499507\n",
      "Current Loss : 82120.14106202868\n",
      "Current Loss : 82111.02701367487\n",
      "Current Loss : 82113.15651871743\n",
      "Current Loss : 82093.42990597934\n",
      "Current Loss : 82093.29042377451\n",
      "Current Loss : 82071.50365711795\n",
      "Current Loss : 82075.70434820217\n",
      "Current Loss : 82070.07017663955\n",
      "Current Loss : 82069.98417294104\n",
      "Current Loss : 82073.36182447168\n",
      "Current Loss : 82073.79138780065\n",
      "Current Loss : 82049.85144010467\n",
      "Current Loss : 82038.28298922663\n",
      "Current Loss : 82016.59050740133\n",
      "Current Loss : 82017.22007247925\n",
      "Current Loss : 81976.6696167716\n",
      "Current Loss : 81982.43261468477\n",
      "Current Loss : 81983.96790203027\n",
      "Current Loss : 81940.20437522067\n",
      "Current Loss : 81953.27143930178\n",
      "Current Loss : 81953.94064763929\n",
      "Current Loss : 81951.91149204705\n",
      "Current Loss : 81943.8099603485\n",
      "Current Loss : 81944.29345317594\n",
      "Current Loss : 81949.32418811912\n",
      "Current Loss : 81949.13804581172\n",
      "Current Loss : 81934.50096459506\n",
      "Current Loss : 81935.21602554557\n",
      "Current Loss : 81924.51012945133\n",
      "Current Loss : 81926.23866915825\n",
      "Current Loss : 81925.48771120803\n",
      "Current Loss : 81926.22261373412\n",
      "Current Loss : 81926.52955003227\n",
      "Current Loss : 81926.80092057791\n",
      "Current Loss : 81927.42731530439\n",
      "Current Loss : 81786.96721054573\n",
      "Current Loss : 81789.93817920765\n",
      "Current Loss : 81789.92930043081\n",
      "Current Loss : 81786.70790479654\n",
      "Current Loss : 81786.48976436781\n",
      "Current Loss : 81751.88900547351\n",
      "Current Loss : 81752.27975667121\n",
      "Current Loss : 81751.34712380577\n",
      "Current Loss : 81749.22252710168\n",
      "Current Loss : 81749.87768472589\n",
      "Current Loss : 81750.2292559776\n",
      "Current Loss : 81741.98929410598\n",
      "Current Loss : 81749.8704363239\n",
      "Current Loss : 81750.32497543406\n",
      "Current Loss : 81751.24521133234\n",
      "Current Loss : 81753.24837255623\n",
      "Current Loss : 81756.70629432591\n",
      "Current Loss : 81749.53469264515\n",
      "Current Loss : 81749.93338107626\n",
      "Current Loss : 81711.70106963365\n",
      "Current Loss : 81714.7891743973\n",
      "Current Loss : 81723.56022154713\n",
      "Current Loss : 81723.17686065727\n",
      "Current Loss : 81722.60656312812\n",
      "Current Loss : 81722.6651395773\n",
      "Current Loss : 81723.32889535418\n",
      "Current Loss : 81703.24151983786\n",
      "Current Loss : 81700.2991400787\n",
      "Current Loss : 81697.14745660932\n",
      "Current Loss : 81697.61634926585\n",
      "Current Loss : 81699.36265550727\n",
      "Current Loss : 81691.44913186425\n",
      "Current Loss : 81694.53898591865\n",
      "Current Loss : 81694.89896779292\n",
      "Current Loss : 81695.08946015492\n",
      "Current Loss : 81686.5205505171\n",
      "Current Loss : 81660.57763610718\n",
      "Current Loss : 81659.93241279345\n",
      "Current Loss : 81660.0886427637\n",
      "Current Loss : 81660.24771074405\n",
      "Current Loss : 81658.820292581\n",
      "Current Loss : 81637.47072299843\n",
      "Current Loss : 81639.43921076413\n",
      "Current Loss : 81645.17228007039\n",
      "Current Loss : 81645.57127659433\n",
      "Current Loss : 81643.95563667083\n",
      "Current Loss : 81644.51292856887\n",
      "Current Loss : 81644.53762794466\n",
      "Current Loss : 81645.60411057918\n",
      "Current Loss : 81640.30609218054\n",
      "Current Loss : 81639.32469147931\n",
      "Current Loss : 81639.5767856706\n",
      "Current Loss : 81614.93632348366\n",
      "Current Loss : 81609.2529002513\n",
      "Current Loss : 81609.14190971144\n",
      "Current Loss : 81609.22284949728\n",
      "Current Loss : 81596.71264159317\n",
      "Current Loss : 81596.27432548274\n",
      "Current Loss : 81592.39710341122\n",
      "Current Loss : 81583.72982388274\n",
      "Current Loss : 81583.68550073654\n",
      "Current Loss : 81559.2021896832\n",
      "Current Loss : 81557.6865723925\n",
      "Current Loss : 81545.26811538689\n",
      "Current Loss : 81543.20884900787\n",
      "Current Loss : 81546.27954802694\n",
      "Current Loss : 81546.88726367425\n",
      "Current Loss : 81545.78874689771\n",
      "Current Loss : 81547.53328135295\n",
      "Current Loss : 81547.81476372496\n",
      "Current Loss : 81547.90743692755\n",
      "Current Loss : 81554.0837683087\n",
      "Current Loss : 81530.84411505915\n",
      "Current Loss : 81438.31960306661\n",
      "Current Loss : 81412.03392029769\n",
      "Current Loss : 81412.49259309728\n",
      "Current Loss : 81409.82918532034\n",
      "Current Loss : 81408.78074734606\n",
      "Current Loss : 81409.0324395422\n",
      "Current Loss : 81408.38178343166\n",
      "Current Loss : 81408.46492001391\n",
      "Current Loss : 81408.2896437642\n",
      "Current Loss : 81409.8247948053\n",
      "Current Loss : 81409.80147227124\n",
      "Current Loss : 81407.55398242384\n",
      "Current Loss : 81401.40356880765\n",
      "Current Loss : 81400.94307666828\n",
      "Current Loss : 81391.59861857047\n",
      "Current Loss : 81386.68307106037\n",
      "Current Loss : 81386.42891722516\n",
      "Current Loss : 81387.74575773557\n",
      "Current Loss : 81373.13639507856\n",
      "Current Loss : 81368.92678929627\n",
      "Current Loss : 81368.42746209734\n",
      "Current Loss : 81367.63970618803\n",
      "Current Loss : 81367.75137437134\n",
      "Current Loss : 81375.76968643426\n",
      "Current Loss : 81374.73618296346\n",
      "Current Loss : 81374.70837064472\n",
      "Current Loss : 81375.63694411262\n",
      "Current Loss : 81371.9708883115\n",
      "Current Loss : 81367.77857532869\n",
      "Current Loss : 81368.7842847745\n",
      "Current Loss : 81369.31043658599\n",
      "Current Loss : 81369.37980434368\n",
      "Current Loss : 81368.3820043319\n",
      "Current Loss : 81370.99719996678\n",
      "Current Loss : 81371.06144675975\n",
      "Current Loss : 81348.19691482926\n",
      "Current Loss : 81338.63513953364\n",
      "Current Loss : 81337.17802788407\n",
      "Current Loss : 81334.93748122959\n",
      "Current Loss : 81335.30052999227\n",
      "Current Loss : 81335.95383113777\n",
      "Current Loss : 81336.40911078446\n",
      "Current Loss : 81336.00628672166\n",
      "Current Loss : 81334.68487802566\n",
      "Current Loss : 81334.26312023589\n",
      "Current Loss : 81333.74879805662\n",
      "Current Loss : 81334.14667573434\n",
      "Current Loss : 81332.88215732333\n",
      "Current Loss : 81317.6289619422\n",
      "Current Loss : 81319.46183238902\n",
      "Current Loss : 81319.75843389258\n",
      "Current Loss : 81320.01036134255\n",
      "Current Loss : 81319.60625075376\n",
      "Current Loss : 81316.05828408587\n",
      "Current Loss : 81315.76979763992\n",
      "Current Loss : 81315.93198462251\n",
      "Current Loss : 81307.77394705825\n",
      "Current Loss : 81307.39973778665\n",
      "Current Loss : 81307.49287715342\n",
      "Current Loss : 81302.08659420155\n",
      "Current Loss : 81302.14128137648\n",
      "Current Loss : 81295.79608013185\n",
      "Current Loss : 81295.91082138309\n",
      "Current Loss : 81296.07694154186\n",
      "Current Loss : 81298.9600118925\n",
      "Current Loss : 81299.85590998922\n",
      "Current Loss : 81299.78193676092\n",
      "Current Loss : 81300.66212381818\n",
      "Current Loss : 81297.60372173355\n",
      "Current Loss : 81297.59610023722\n",
      "Current Loss : 81298.04947685139\n",
      "Current Loss : 81298.48454655528\n",
      "Current Loss : 81298.3237206535\n",
      "Current Loss : 81298.38696065755\n",
      "Current Loss : 81297.67935073767\n",
      "Current Loss : 81298.12856013025\n",
      "Current Loss : 81295.78315879317\n",
      "Current Loss : 81296.21214860052\n",
      "Current Loss : 81294.52395795195\n",
      "Current Loss : 81285.02941000518\n",
      "Current Loss : 81284.0137070347\n",
      "Current Loss : 81284.18144957989\n",
      "Current Loss : 81284.03235505434\n",
      "Current Loss : 81274.4667567391\n",
      "Current Loss : 81274.274205781\n",
      "Current Loss : 81271.28019484939\n",
      "Current Loss : 81261.58967339405\n",
      "Current Loss : 81262.09157292903\n",
      "Current Loss : 81259.88718766597\n",
      "Current Loss : 81266.13886550658\n",
      "Current Loss : 81266.8453295735\n",
      "Current Loss : 81268.27481473224\n",
      "Current Loss : 81269.04863092196\n",
      "Current Loss : 81269.35054414865\n",
      "Current Loss : 81240.81056321695\n",
      "Current Loss : 81241.14941506204\n",
      "Current Loss : 81239.18782692803\n",
      "Current Loss : 81229.73743637976\n",
      "Current Loss : 81227.82731934766\n",
      "Current Loss : 81227.80835752822\n",
      "Current Loss : 81227.8561162822\n",
      "Current Loss : 81226.90160819906\n",
      "Current Loss : 81227.0280627307\n",
      "Current Loss : 81224.81693457505\n",
      "Current Loss : 81233.01935122393\n",
      "Current Loss : 81211.00772320162\n",
      "Current Loss : 81184.82612094341\n",
      "Current Loss : 81184.72856486788\n",
      "Current Loss : 81188.75243222583\n",
      "Current Loss : 81189.11635616205\n",
      "Current Loss : 81169.8647055422\n",
      "Current Loss : 81151.15703441147\n",
      "Current Loss : 81151.7629623641\n",
      "Current Loss : 81151.56314440745\n",
      "Current Loss : 81142.91641558532\n",
      "Current Loss : 81142.91524667547\n",
      "Current Loss : 81143.03519368896\n",
      "Current Loss : 81144.47210028226\n",
      "Current Loss : 81066.99332719974\n",
      "Current Loss : 81062.90148305376\n",
      "Current Loss : 81063.3153223209\n",
      "Current Loss : 81063.46188764898\n",
      "Current Loss : 81063.95130904562\n",
      "Current Loss : 81052.64502929847\n",
      "Current Loss : 81033.98930744713\n",
      "Current Loss : 81037.66794730844\n",
      "Current Loss : 81037.37458128881\n",
      "Current Loss : 81037.48925164969\n",
      "Current Loss : 81037.88191649057\n",
      "Current Loss : 81037.89717723019\n",
      "Current Loss : 81039.21949935007\n",
      "Current Loss : 81039.11250915153\n",
      "Current Loss : 81027.59146473455\n",
      "Current Loss : 81027.52053261107\n",
      "Current Loss : 81027.74321108915\n",
      "Current Loss : 81028.12680893499\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[125], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1000\u001b[39m): \u001b[39m# 1000 iterations\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     curr_c \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m,k\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCurrent Loss : \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(cost_function(X_0, A)))\n\u001b[1;32m     11\u001b[0m     \u001b[39m# Gradient step\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     X_0[:, curr_c] \u001b[39m=\u001b[39m X_0[:, curr_c] \u001b[39m-\u001b[39m \u001b[39m0.01\u001b[39m \u001b[39m*\u001b[39m partial_gradient(curr_c) \n",
      "Cell \u001b[0;32mIn[76], line 18\u001b[0m, in \u001b[0;36mcost_function\u001b[0;34m(X, A)\u001b[0m\n\u001b[1;32m     16\u001b[0m     current \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39m# so we have the correct start value\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[39mfor\u001b[39;00m label_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(k):\n\u001b[0;32m---> 18\u001b[0m         current \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexp(np\u001b[39m.\u001b[39mdot((X[:,label_idx])\u001b[39m.\u001b[39mT, A[sample_idx,:]))\n\u001b[1;32m     22\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39meinsum(\u001b[39m'\u001b[39m\u001b[39mij,ji->i\u001b[39m\u001b[39m'\u001b[39m, X_l, A)) \u001b[39m+\u001b[39m final_sum\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "X_0 = X\n",
    "# Calculate Loss in the beginning\n",
    "\n",
    "#loss = cost_function(X_0, A)\n",
    "\n",
    "\n",
    "for i in range(1000): # 1000 iterations\n",
    "    curr_c = random.randint(0,k-1)\n",
    "    print(\"Current Loss : {}\".format(cost_function(X_0, A)))\n",
    "    # Gradient step\n",
    "\n",
    "    \n",
    "    X_0[:, curr_c] = X_0[:, curr_c] - 0.01 * partial_gradient(curr_c) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
