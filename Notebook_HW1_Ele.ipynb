{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization for Data Science 2024 Homework 1\n",
    "\n",
    "Students:\n",
    "\n",
    "Alberto Calabrese Nº:2103405\n",
    "\n",
    "Greta d'Amore Grelli Nº:2122424\n",
    "\n",
    "Eleonora Mesaglio Nº:2103402\n",
    "\n",
    "Marlon Helbing Nº:2106578"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set a seed for deterministic outputs\n",
    "SEED = 0\n",
    "np.random.seed(seed = SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A - MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 1000\n",
    "NUM_FEATURES = 1000\n",
    "\n",
    "# Generate a 1000x1000 matrix with random samples from a standard normal distribution\n",
    "# This is our data matrix, which contains 1000 samples (rows) with 1000 features each (columns)\n",
    "# A MATRIX\n",
    "data_matrix = np.random.normal(0, 1, size = (NUM_SAMPLES, NUM_FEATURES)) # He refers to it as A\n",
    "A = data_matrix \n",
    "# Now 'data_matrix' contains random values drawn from N(0,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X - MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.51424689  1.11286451  0.67502449 ...  0.25882336 -1.19061855\n",
      "   0.33358561]\n",
      " [-1.81915062 -0.47212146 -0.34912377 ...  0.62585093 -0.65290049\n",
      "   0.80570443]\n",
      " [-0.55865306 -0.0035541   0.24477089 ... -0.26494581 -0.44695689\n",
      "  -1.48747178]\n",
      " ...\n",
      " [ 2.33758546 -0.84650435  0.79761719 ...  0.23849016  1.6671718\n",
      "  -0.85339979]\n",
      " [ 1.03501666  0.69079556 -1.11256574 ...  0.08100798 -1.28175412\n",
      "  -1.45574687]\n",
      " [-1.17802047 -0.58835553 -1.21609739 ... -0.39206131 -0.97551536\n",
      "  -1.14216452]]\n"
     ]
    }
   ],
   "source": [
    "NUM_LABELS = 50\n",
    "\n",
    "# This is our weight matrix that we initialize like this ; these weights we want to learn\n",
    "# it has 1000 features (rows) with 50 labels each (columns)\n",
    "# X MATRIX\n",
    "weight_matrix = np.random.normal(0, 1, size = (NUM_FEATURES, NUM_LABELS)) # He refers to it as X \n",
    "X = weight_matrix\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E - MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EXAMPLES = 1000\n",
    "\n",
    "# This matrix is used to help generating our supervised gold labels \n",
    "# It is of size 1000 training examples (rows) and their labels (columns)\n",
    "generative_matrix = np.random.normal(0, 1, size = (NUM_EXAMPLES, NUM_LABELS)) # He refers to it as E \n",
    "E = generative_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AX + E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector with numbers from 1 to 50\n",
    "label_vector = np.arange(1, 51)\n",
    "\n",
    "# Print the vector\n",
    "#print(label_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 28.37955025  12.44004627  74.53765602 ...   5.36347241  29.24640324\n",
      "  -58.68832687]\n",
      " [-36.84008464  -1.55276569 -35.44585854 ...  10.0214222  -13.82201066\n",
      "   -8.29253847]\n",
      " [ 28.64849189  -6.52245633 -31.13462845 ...  73.6132841  -10.96331242\n",
      "  -53.34722266]\n",
      " ...\n",
      " [ 16.19559425  55.91715705  15.65062663 ... -15.05409478  35.96461763\n",
      "   39.89412112]\n",
      " [ 30.58357539   2.84910196 -34.83950691 ...  -3.98929294  -9.25761593\n",
      "   13.50727109]\n",
      " [ -7.68284097  20.49135152  30.88517547 ... -40.13266463  10.54610989\n",
      "  -44.44748546]]\n",
      "(1000, 50)\n"
     ]
    }
   ],
   "source": [
    "# Now he wants us to calculate AX+E to generate labels for the 1000 training examples (such that we have a supervised learning set) +\n",
    "\n",
    "# Calculate the matrix product AX\n",
    "AX = np.matmul(data_matrix, weight_matrix)  # or simply: AX = A @ X\n",
    "print(AX)\n",
    "\n",
    "# Add E to AX element-wise\n",
    "result_matrix = AX + generative_matrix\n",
    "\n",
    "print(result_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MAX INDEX AS CLASS LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2 38 47 33 24 35 22 18 30 49  2 42 23 22  0 29 47 40 30 13 32 23 36  9\n",
      " 29 14  3 38  6 21 37  4 34 39 32 22 12  9 31 27 21 16 24  7 27 39 21 35\n",
      "  8 39 25 27 48 48 46  6 46 37  1 49 27 46 11 25 47 49 24 45 28 27 48 37\n",
      " 21 23  1  3 45 27 18  8 19 11 40 16 10 46  0 19 46 21 10 13 13 13 15 16\n",
      "  7 22  8 14 12 49  8 43 21 17 45 24 17 19 23  3 10  6 10 12 17 29 48 48\n",
      " 22 26  4 27 41 20 46 20 33 30 20 13 44 16 20 35 39  7 16 13 41 48 32 40\n",
      " 44 14 41 37 43 46 48 30 34 38  8 15  9 16  9 23 31 15 27 11 36 25 27  2\n",
      " 12 29 17 14 39 20 47 41 21 14 20 10 23  1 45  2 15 27  5 20 31 23 25 49\n",
      " 33  4 46 31 27 27  6 45 27 30 40 39  9  5 33 32 46 49 41 41 40  7  0 26\n",
      " 19 43 12  0 32 31 13 39 14 22 14 34  8 27  0 26 24 14 25 37 37 32 25 40\n",
      " 11 39 15 25 39 47 17 49 27 25 41 42 25  2 31 49 16 48 38 20 25  4 47 18\n",
      " 12 44 36  7  8 29 41 45 34  4 10 21 11 38 32 41 26 29 15  1 16  6 11 10\n",
      " 17  3 43 36 22 23 11 24 34  0 32 24 18 45 16  3  5 15 25 17 16 20  0 17\n",
      " 36 20 46 46  2 42 45 41 36 28 45 37 37 39 26 21 45 38 45 17 27 44 37 27\n",
      " 12 37 23  6  6 36 42 48 14 34 41  3  3 38 25 32 13  0 33 40 11 23 24 28\n",
      " 20 13 18 42  6  9 39 29  8 23  8 24  8 30 15 26 25  5 40 32 22 39 15 45\n",
      " 21 34 22 11 29 44  3 21 12  4  9 23 32  2 33 24 48 25 23 11 25 15 37 23\n",
      " 41  7  8 49  2  2 45 38 33 22 26  4 26 16 13 40 13 36 44 11 40 35 48  7\n",
      " 34 32 43 46 44  1 24 35 47  3 49  5 29 43 38 40 30  6 49 16 45 43 34  0\n",
      "  4  8  9  9 12 48 17 19 20  2  6  6 25 46 39 41  9 14 25 43 40 30  5 46\n",
      " 29 31  1 42 26 27 36 24 12 11 42 41 38 36 29 45 35  6 24 31  6 22  2 24\n",
      " 11 10 16  9 16 26 31  6  3 22  8  3 20 26 47 46 16 40 37 48 32 27 33  2\n",
      " 26 45 12 48 44  4 13 40 37 39 25 36  2 43 27 12 41 49 34 48  3 46 44  3\n",
      "  3 27 46 48 39 18 44 33 35 28  3  9 27 16 22 37 45 10 17 40 41 11  6 26\n",
      " 22 12 41 47 31  8 38 29 10 42  8 45 28 35  4 21 19 48 25 45  4 16  3 43\n",
      " 40  6 20 21 15 41 37 31 41 18  9 44 32 16 41 35 36 21  0 30 38 37  6 24\n",
      " 16 20 32 35 42  6 37  2 14 49  3  1 11 10  0 48 25 28 20 21 16 41 43  1\n",
      " 38  2 25 30  4  2 37 20 46 16 36 35 47 25 38 33 25 32 38  9  0  2 11 15\n",
      "  5 25  3 17 22 45 30 48 15 33 21 46 31 14 43  1 27 32 28 47 25 14 21 22\n",
      " 37 20 40 10 23 32  0 26 47 11 38 35  8 18 20 45 27 33 11 21 33 28  2 41\n",
      "  1 14  5 37 47 24 41 25 10 21 13  2  6 45 43 46 33  8 14 49 13  8 42 37\n",
      " 39  8 45 39 29  6 10 21 13  2 44 16 10  7 38 28 27 21 13 11 18  4 49  4\n",
      " 10  9 44 16 10 26 32 34 34 37 22 42 28 46 31  3 14 21  8 42 15 46  3 47\n",
      " 30 40  6 13 34 46 24 35 35  8 15 31 21  2 40 26 30 28  0 45 18 17 45  2\n",
      " 28 44 22 27 36 41 40 13 30 41 44 41 11  6 21 14 31 44  3 49  7 36 15 31\n",
      " 43 25  8 25  0  6 39  8 40 27  5  5 41 48  6  2 46 45 22 22 25 13  0 31\n",
      " 10 17  1 10 13 23 11 34 32  3  4 22 30 37 42  9 27 42 33 47 34 39 34 47\n",
      " 24 48 28 32 13 14  1 15 24  0 41 26  6 27  7 18  6 35 25  6 26 10 30 30\n",
      " 36 33 13 39 18 10 40 20 20 23 35 15  0  3 23 36 36 34 36  9  8  5 14 45\n",
      " 30 37  9 46  8 49  2  5 27  5 17 10 26 32  1  4 10 23  9  7 41 32 14 16\n",
      " 44 22 13 30 13 45 39 43 43 33 13 41 38 11 13 22 24 14 40 16  7 37 35 33\n",
      " 27 40 25 40 20 23 25 42 47  8  3 47 13 22 25 45]\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "# We find our labels by considering the max index in the row as the class label\n",
    "\n",
    "# Find the column indices of maximum values for each row\n",
    "labels = np.argmax(result_matrix, axis=1)\n",
    "\n",
    "print(labels)\n",
    "\n",
    "#print(result_matrix[2,:])\n",
    "print(labels.shape)\n",
    "\n",
    "# 'max_indices' now contains the column indices of maximum values for each row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 1000)\n"
     ]
    }
   ],
   "source": [
    "A_train = A[0:800,:]\n",
    "A_test = A[800:1000, :]\n",
    "\n",
    "print(A_test.shape)\n",
    "labels_train = labels[0:800]\n",
    "labels_test = labels[800:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the negative log-likelihood function\n",
    "def cost_func(data_matrix, weight_matrix, labels):\n",
    "    scores = np.dot(data_matrix, weight_matrix)\n",
    "    exp_scores = np.exp(scores)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    corect_logprobs = -np.log(probs[range(NUM_EXAMPLES), labels])\n",
    "    data_loss = np.sum(corect_logprobs)\n",
    "    return data_loss\n",
    "\n",
    "\n",
    "# Define the function to compute the gradient of the negative log-likelihood function\n",
    "def gradient(data_matrix, weight_matrix, labels):\n",
    "    scores = np.dot(data_matrix, weight_matrix)\n",
    "    exp_scores = np.exp(scores)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    probs[range(NUM_EXAMPLES), labels] -= 1\n",
    "    dW = np.dot(data_matrix.T, probs)\n",
    "    return dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 4.146766\n",
      "iteration 100: loss 0.207946\n",
      "iteration 200: loss 0.116930\n",
      "iteration 300: loss 0.082825\n",
      "iteration 400: loss 0.064673\n",
      "iteration 500: loss 0.053311\n",
      "iteration 600: loss 0.045492\n",
      "iteration 700: loss 0.039762\n",
      "iteration 800: loss 0.035373\n",
      "iteration 900: loss 0.031897\n"
     ]
    }
   ],
   "source": [
    "# Define the learning rate and the number of iterations\n",
    "learning_rate = 0.01\n",
    "num_iterations = 1000\n",
    "\n",
    "# Perform gradient descent\n",
    "for i in range(num_iterations):\n",
    "    grad = gradient(data_matrix, weight_matrix, max_indices)\n",
    "    weight_matrix -= learning_rate * grad\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(\"iteration %d: loss %f\" % (i, cost_func(data_matrix, weight_matrix, max_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LABELED = 500\n",
    "Y_0 = np.random.rand(NUM_SAMPLES, NUM_LABELS) # define an appropriate starting point\n",
    "assert Y_0.shape == (NUM_SAMPLES, NUM_LABELS)\n",
    "\n",
    "EPSILON = 1e-6 # define small epsilon for stopping criterion\n",
    "MAX_ITER = 2000 # and/or a maximum number of iterations (or even a maximum time)\n",
    "\n",
    "ALPHA = 0.01 # define a fixed stepsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "Y_iterates = [Y_0]\n",
    "times = [0]\n",
    "start = time.time()\n",
    "\n",
    "grad = gradient(data_matrix, weight_matrix, max_indices)\n",
    "while len(Y_iterates) < MAX_ITER and np.linalg.norm(grad) > EPSILON: # TO DO: write the condition for the while loop\n",
    "    new_y = Y_iterates[-1] - ALPHA * grad # write the update\n",
    "    Y_iterates.append(new_y)\n",
    "    times.append(time.time() - start)\n",
    "    # Check the stopping criterion\n",
    "    grad = gradient(data_matrix, weight_matrix, max_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "shape mismatch: indexing arrays could not be broadcast together with shapes (1000,) (1000,50) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(times, [cost_func(data_matrix, weight_matrix, y\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m Y_iterates])\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCPU time (seconds)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObjective function\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(times, [cost_func(data_matrix, weight_matrix, y\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m Y_iterates])\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCPU time (seconds)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObjective function\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m, in \u001b[0;36mcost_func\u001b[0;34m(data_matrix, weight_matrix, labels)\u001b[0m\n\u001b[1;32m      4\u001b[0m exp_scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(scores)\n\u001b[1;32m      5\u001b[0m probs \u001b[38;5;241m=\u001b[39m exp_scores \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(exp_scores, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 6\u001b[0m corect_logprobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mlog(probs[\u001b[38;5;28mrange\u001b[39m(NUM_EXAMPLES), labels])\n\u001b[1;32m      7\u001b[0m data_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(corect_logprobs)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data_loss\n",
      "\u001b[0;31mIndexError\u001b[0m: shape mismatch: indexing arrays could not be broadcast together with shapes (1000,) (1000,50) "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(times, [cost_func(data_matrix, weight_matrix, y.astype(int)) for y in Y_iterates])\n",
    "plt.xlabel('CPU time (seconds)')\n",
    "plt.ylabel('Objective function')\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. BCGD with Randomized Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define as a block a single column in the parameter matrix $X$. Thus, one block defines all features for a single class. As this is a column vector in the matrix $X$, our partial gradient is now only dependent on $c$ (because we have a gradient for all the features of one class).\n",
    "\n",
    "\n",
    "Our partial derivative for one block then looks like the following\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial f(X)}{\\partial X_{c}} = - A^{T} \\cdot (L^{I} - Q) = A^{T} \\cdot (- L^{I} + Q)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "where \n",
    "\n",
    "$A$ has form $m \\times d$ ; it is our given matrix A.\n",
    "\\\n",
    "$L^{I}$ has form $m \\times 1$ ; it is the indicator vector containing $1$'s only at the positions where the label of the current sample $i$ is equal to $c$ and $0$'s everywhere else.\n",
    "$$\n",
    "L_{i}^{I}=\\begin{cases}\n",
    "\t\t\t1 \\quad & \\text{if $label_{i} = c $}\\\\\n",
    "            0 \\quad & \\text{otherwise}\n",
    "\t\t \\end{cases}\n",
    "$$\n",
    "\\\n",
    "$Q$ has form $m \\times 1$ ; it is the vector calculating the exponential expression $\\dfrac{\\exp(x_{c}^{T}a_{i})}{\\sum_{c' = 1}^{k} \\exp(x_{c'}^{T}a_{i}) }$ for each sample $i$\n",
    "\n",
    "Thus, our result will be of form $d \\times 1$\n",
    "\n",
    "Note that the calculations needed for $L^{I}$ and $Q$ only depend on $c$ and $i$ . However, as we know all the samples $i$, we construct the vectors $L^{I}$ and $Q$ which are then only dependent on $c$ .\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### NOT TESTED YET ########\n",
    "m = 1000 # samples\n",
    "d = 1000 # features\n",
    "k = 50   # labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(X,A):\n",
    "    # X_l is m x d, where each row is a column of X and depends on what label the sample has\n",
    "    # for example, the first row of X_l is the column vector of matrix X at the index of the label that sample 1 has\n",
    "    X_l = np.zeros((m,d))\n",
    "    # Iterate over all labels and notice that we have a label for each sample, thus we can use idx directly\n",
    "    for idx,label in enumerate(labels):\n",
    "        X_l[idx,:] = X[:,label]\n",
    "\n",
    "    # Make negative\n",
    "    X_l = -1 * X_l\n",
    "    # Now we have to manually calculate the double sum \n",
    "    final_sum = 0\n",
    "    current = 1 # initial so np.log(1) = 0\n",
    "    for sample_idx in range(m):\n",
    "        final_sum += np.log(current)\n",
    "        current = 0 # so we have the correct start value\n",
    "        for label_idx in range(k):\n",
    "            current += np.exp(np.dot((X[:,label_idx]).T, A[sample_idx,:]))\n",
    "\n",
    "  \n",
    " \n",
    "    return np.sum(np.einsum('ij,ji->i', X_l, A)) + final_sum\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function_full(X,A):\n",
    "    # This function is ugly but just for testing if we did smth wrong\n",
    "    sum_1 = 0\n",
    "    for sample_idx in range(m):\n",
    "        # Take the column at index of the label of the current sample\n",
    "        x_bi = X[:, labels[sample_idx]]\n",
    "        x_bi = -1 * x_bi\n",
    "        a_i = A[sample_idx, :]\n",
    "        sum_1 += (x_bi @ a_i) # automatically calculates (1,1000) x (1000,1)\n",
    "    sum_2 = 0\n",
    "    for sample_idx in range(m):\n",
    "        current = 0 # so we have the correct start value\n",
    "        for label_idx in range(k):\n",
    "            current += np.exp((X[:,label_idx]) @ A[sample_idx,:])\n",
    "        sum_2 += np.log(current)\n",
    "\n",
    "    return sum_1 + sum_2 \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 1000 # samples\n",
    "d = 1000 # features\n",
    "k = 50   # labels\n",
    "\n",
    "def partial_gradient(X,c):\n",
    "    # We need X as parameter so it changes value across diff gradients\n",
    "\n",
    "    # We define the partial gradient\n",
    "    \n",
    "    # Calculating indicator vector L \n",
    "\n",
    "    # Initialize empty L in size of all samples (=1000)\n",
    "    L = np.zeros((m,1))\n",
    "\n",
    "    # Iterate over labels of each sample\n",
    "    for idx,label in enumerate(labels):\n",
    "        # If there is a label match\n",
    "        if label == c:\n",
    "            # We assign a 1\n",
    "            L[idx] = 1\n",
    "        # If there is no match\n",
    "        else:\n",
    "            # We assign a 0\n",
    "            L[idx] = 0\n",
    "\n",
    "    # Calculating vector Q\n",
    "\n",
    "    # Initialize empty Q in size of all samples (=1000)\n",
    "    \n",
    "    Q = np.zeros((m,1))\n",
    "\n",
    "        \n",
    "    # Iterate over all samples\n",
    "    for curr_sample in range(m):\n",
    "    \n",
    "        nominator = np.exp((X[:,c]) @ (A[curr_sample,:]))\n",
    "    \n",
    "        denominator = 0\n",
    "        # Iterate over all labels for the denominator\n",
    "        for curr_label in range(k):\n",
    "            denominator += np.exp((X[:,curr_label]) @ A[curr_sample,:])\n",
    "    \n",
    "       \n",
    "        Q[curr_sample] = nominator/denominator\n",
    "\n",
    "\n",
    "\n",
    "    return (np.dot(A.T, ((-1 * L) + Q))).squeeze() # returns (1000,)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_gradient(X):\n",
    "    # initialize zero gradient of size (m,)\n",
    "    grad = np.zeros(m)\n",
    "    for label in range(k):\n",
    "        grad = np.column_stack((grad, partial_gradient(X,label)))\n",
    "\n",
    "  \n",
    "\n",
    "    return grad[:,1:] #remove 0 column\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 51)\n",
      "(1000, 50)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss : 67.33816574211232\n",
      "Current Loss : 66.8656036343018\n",
      "Current Loss : 65.9793249854265\n",
      "Current Loss : 65.53005148608645\n",
      "Current Loss : 64.98676737524511\n",
      "Current Loss : 64.41677674457605\n",
      "Current Loss : 63.72506138010067\n",
      "Current Loss : 63.392115071023\n",
      "Current Loss : 63.23957716625591\n",
      "Current Loss : 62.93268045304285\n",
      "Current Loss : 62.11668297099823\n",
      "Current Loss : 61.50239444996987\n",
      "Current Loss : 61.38610857719323\n",
      "Current Loss : 60.469223378473544\n",
      "Current Loss : 60.093481244082795\n",
      "Current Loss : 59.8803582911205\n",
      "Current Loss : 59.668645929326885\n",
      "Current Loss : 58.55592643730051\n",
      "Current Loss : 56.925842263692175\n",
      "Current Loss : 56.57033450152085\n",
      "Current Loss : 56.41408288483217\n",
      "Current Loss : 56.146999845062965\n",
      "Current Loss : 55.83272924195626\n",
      "Current Loss : 55.430736605019774\n",
      "Current Loss : 55.293340426447685\n",
      "Current Loss : 54.8427886415011\n",
      "Current Loss : 54.4924072536669\n",
      "Current Loss : 54.281784329446964\n",
      "Current Loss : 53.75922115208232\n",
      "Current Loss : 53.31010898106615\n",
      "Current Loss : 52.39503035113739\n",
      "Current Loss : 52.2624433748133\n",
      "Current Loss : 51.705552721978165\n",
      "Current Loss : 51.08208483090857\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[127], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m500\u001b[39m): \u001b[39m# 1000 iterations\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     curr_c \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m,k\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCurrent Loss : \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(cost_function_full(X_0, A)))\n\u001b[1;32m      8\u001b[0m     \u001b[39m# Gradient step\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     X_0[:, curr_c] \u001b[39m=\u001b[39m X_0[:, curr_c] \u001b[39m-\u001b[39m \u001b[39m0.001\u001b[39m \u001b[39m*\u001b[39m partial_gradient(X_0,curr_c) \n",
      "Cell \u001b[0;32mIn[121], line 14\u001b[0m, in \u001b[0;36mcost_function_full\u001b[0;34m(X, A)\u001b[0m\n\u001b[1;32m     12\u001b[0m     current \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39m# so we have the correct start value\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[39mfor\u001b[39;00m label_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(k):\n\u001b[0;32m---> 14\u001b[0m         current \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mexp((X[:,label_idx]) \u001b[39m@\u001b[39;49m A[sample_idx,:])\n\u001b[1;32m     15\u001b[0m     sum_2 \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlog(current)\n\u001b[1;32m     17\u001b[0m \u001b[39mreturn\u001b[39;00m sum_1 \u001b[39m+\u001b[39m sum_2\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "X_0 = X\n",
    "# Calculate Loss in the beginning\n",
    "\n",
    "\n",
    "\n",
    "for i in range(500): # 1000 iterations\n",
    "    curr_c = random.randint(0,k-1)\n",
    "    print(\"Current Loss : {}\".format(cost_function_full(X_0, A)))\n",
    "    # Gradient step\n",
    "\n",
    "    X_0[:, curr_c] = X_0[:, curr_c] - 0.001 * partial_gradient(X_0,curr_c) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BCGD Gauss-Southwell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to calculate the partial gradient of each block ; We calculate it in our partial_gradient function. Thus, we have to iterate over all possible blocks\n",
    "# , which in our case are class - many, so k many \n",
    "\n",
    "\n",
    "def Gauss_Southwell(X):\n",
    "\n",
    "    learning_rate = 0.01 # TODO : REPLACE BY LIPSCHITZ\n",
    "    X_0 = X\n",
    "    for it in range(20): # iterations\n",
    "        \n",
    "        all_partial_gradients = []\n",
    "        for label_idx in range(k): # k is number of all labels\n",
    "            # Calculate the partial gradient of each block\n",
    "            all_partial_gradients.append(partial_gradient(X_0,label_idx))\n",
    "\n",
    "        # Gradients will be of size (features,) so in our case (1000,) \n",
    "        all_partial_gradients_norms = [np.linalg.norm(curr_grad) for curr_grad in all_partial_gradients]\n",
    "\n",
    "        \n",
    "        max_idx = np.argmax(all_partial_gradients_norms)\n",
    "   \n",
    "    \n",
    "\n",
    "        partial_grad = all_partial_gradients[max_idx]\n",
    "      \n",
    "\n",
    "        # Gradient Descent \n",
    "         \n",
    "        print(\"Current Loss : {}\".format(cost_function_full(X_0, A)))\n",
    "        # Gradient step\n",
    "        X_0[:, max_idx] = X_0[:, max_idx] - learning_rate * partial_grad\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss : 48.8254584602837\n",
      "Current Loss : 44.60292529145954\n",
      "Current Loss : 40.827469026946346\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[140], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m Gauss_Southwell(X)\n",
      "Cell \u001b[0;32mIn[139], line 14\u001b[0m, in \u001b[0;36mGauss_Southwell\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     11\u001b[0m all_partial_gradients \u001b[39m=\u001b[39m []\n\u001b[1;32m     12\u001b[0m \u001b[39mfor\u001b[39;00m label_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(k): \u001b[39m# k is number of all labels\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[39m# Calculate the partial gradient of each block\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     all_partial_gradients\u001b[39m.\u001b[39mappend(partial_gradient(X_0,label_idx))\n\u001b[1;32m     16\u001b[0m \u001b[39m# Gradients will be of size (features,) so in our case (1000,) \u001b[39;00m\n\u001b[1;32m     17\u001b[0m all_partial_gradients_norms \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(curr_grad) \u001b[39mfor\u001b[39;00m curr_grad \u001b[39min\u001b[39;00m all_partial_gradients]\n",
      "Cell \u001b[0;32mIn[117], line 41\u001b[0m, in \u001b[0;36mpartial_gradient\u001b[0;34m(X, c)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[39m# Iterate over all labels for the denominator\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[39mfor\u001b[39;00m curr_label \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(k):\n\u001b[0;32m---> 41\u001b[0m         denominator \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mexp((X[:,curr_label]) \u001b[39m@\u001b[39;49m A[curr_sample,:])\n\u001b[1;32m     44\u001b[0m     Q[curr_sample] \u001b[39m=\u001b[39m nominator\u001b[39m/\u001b[39mdenominator\n\u001b[1;32m     48\u001b[0m \u001b[39mreturn\u001b[39;00m (np\u001b[39m.\u001b[39mdot(A\u001b[39m.\u001b[39mT, ((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39m*\u001b[39m L) \u001b[39m+\u001b[39m Q)))\u001b[39m.\u001b[39msqueeze()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Gauss_Southwell(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss : 69.78012747403409\n",
      "Current Loss : 39.52945970639121\n",
      "Current Loss : 28.02260797996132\n",
      "Current Loss : 22.274046466394793\n",
      "Current Loss : 18.72851959388936\n",
      "Current Loss : 16.28161745829857\n",
      "Current Loss : 14.472780104231788\n",
      "Current Loss : 13.071795664829551\n",
      "Current Loss : 11.949352403433295\n",
      "Current Loss : 11.026623155732523\n",
      "Current Loss : 10.25254239696369\n",
      "Current Loss : 9.592432513367385\n",
      "Current Loss : 9.021840558067197\n",
      "Current Loss : 8.522983455724898\n",
      "Current Loss : 8.08259155297128\n",
      "Current Loss : 7.690544687167858\n",
      "Current Loss : 7.338978669780772\n",
      "Current Loss : 7.021682103222702\n",
      "Current Loss : 6.733678491043975\n",
      "Current Loss : 6.470930076626246\n",
      "Current Loss : 6.230123722096323\n",
      "Current Loss : 6.008513320019119\n",
      "Current Loss : 5.803801961854333\n",
      "Current Loss : 5.614052559205447\n",
      "Current Loss : 5.437619167350931\n",
      "Current Loss : 5.273093592768419\n",
      "Current Loss : 5.119263429616694\n",
      "Current Loss : 4.9750787572265835\n",
      "Current Loss : 4.839625467124279\n",
      "Current Loss : 4.712103721889434\n",
      "Current Loss : 4.591810420897673\n",
      "Current Loss : 4.4781248257058905\n",
      "Current Loss : 4.370496696108603\n",
      "Current Loss : 4.268436436337652\n",
      "Current Loss : 4.171506863218383\n",
      "Current Loss : 4.079316289557028\n",
      "Current Loss : 3.9915126849955413\n",
      "Current Loss : 3.9077787203568732\n",
      "Current Loss : 3.8278275431512157\n",
      "Current Loss : 3.7513991611631354\n",
      "Current Loss : 3.678257331936038\n",
      "Current Loss : 3.608186880199355\n",
      "Current Loss : 3.5409913718758617\n",
      "Current Loss : 3.476491093271761\n",
      "Current Loss : 3.4145212880102918\n",
      "Current Loss : 3.354930614688783\n",
      "Current Loss : 3.29757979180431\n",
      "Current Loss : 3.2423404065630166\n",
      "Current Loss : 3.1890938626311254\n",
      "Current Loss : 3.137730447939248\n",
      "Current Loss : 3.0881485088466434\n",
      "Current Loss : 3.0402537157788174\n",
      "Current Loss : 2.9939584057137836\n",
      "Current Loss : 2.9491809989413014\n",
      "Current Loss : 2.9058454725454794\n",
      "Current Loss : 2.86388088997046\n",
      "Current Loss : 2.8232209791895\n",
      "Current Loss : 2.7838037492183503\n",
      "Current Loss : 2.745571147155715\n",
      "Current Loss : 2.708468746059225\n",
      "Current Loss : 2.672445464093471\n",
      "Current Loss : 2.637453307121177\n",
      "Current Loss : 2.6034471367747756\n",
      "Current Loss : 2.5703844596864656\n",
      "Current Loss : 2.5382252328126924\n",
      "Current Loss : 2.5069316882581916\n",
      "Current Loss : 2.476468172040768\n",
      "Current Loss : 2.4468009965203237\n",
      "Current Loss : 2.417898305080598\n",
      "Current Loss : 2.3897299478412606\n",
      "Current Loss : 2.3622673681675224\n",
      "Current Loss : 2.3354834972997196\n",
      "Current Loss : 2.309352657161071\n",
      "Current Loss : 2.2838504723767983\n",
      "Current Loss : 2.2589537862950237\n",
      "Current Loss : 2.2346405860298546\n",
      "Current Loss : 2.2108899316808674\n",
      "Current Loss : 2.187681890616659\n",
      "Current Loss : 2.1649974775209557\n",
      "Current Loss : 2.1428185978875263\n",
      "Current Loss : 2.121127995866118\n",
      "Current Loss : 2.0999092053243658\n",
      "Current Loss : 2.07914650553721\n",
      "Current Loss : 2.058824878098676\n",
      "Current Loss : 2.038929968388402\n",
      "Current Loss : 2.019448048711638\n",
      "Current Loss : 2.0003659835929284\n",
      "Current Loss : 1.9816711988096358\n",
      "Current Loss : 1.9633516502362909\n",
      "Current Loss : 1.9453957971709315\n",
      "Current Loss : 1.9277925748756388\n",
      "Current Loss : 1.9105313709296752\n",
      "Current Loss : 1.8936020012624795\n",
      "Current Loss : 1.8769946894899476\n",
      "Current Loss : 1.8607000454794616\n",
      "Current Loss : 1.8447090467670932\n",
      "Current Loss : 1.8290130211680662\n",
      "Current Loss : 1.8136036281794077\n",
      "Current Loss : 1.798472845228389\n",
      "Current Loss : 1.7836129500210518\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "X_0 = X\n",
    "gradient_norms = []\n",
    "for i in range(100): # 10 iterations\n",
    "    print(\"Current Loss : {}\".format(cost_function_full(X_0, A)))\n",
    "    # Gradient step\n",
    "\n",
    "    # Calculate current gradient\n",
    "    gradient_at_i = full_gradient(X_0)\n",
    "    # Calculate the norm\n",
    "    gradient_norms.append(np.linalg.norm(gradient_at_i))\n",
    "    # Gradient step\n",
    "    X_0 = X_0 - 0.001 * gradient_at_i\n",
    "\n",
    "\n",
    "# Create an array of iteration numbers\n",
    "iterations = np.arange(len(gradient_norms))\n",
    "\n",
    "# Create Gradient Norm Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(iterations, gradient_norms, label='Gradient Norm')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Gradient Norm')\n",
    "plt.title('Gradient Norm across Iterations')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# See how X_0 performs :\n",
    "\n",
    "result_matrix_descent = A @ X_0\n",
    "\n",
    "\n",
    "# Find the column indices of maximum values for each row\n",
    "labels_descent = np.argmax(result_matrix_descent, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "equal_entries = labels == labels_descent\n",
    "\n",
    "# Count the number of True values (equal entries)\n",
    "num_equal_entries = np.sum(equal_entries)\n",
    "\n",
    "# Accuracy :\n",
    "accuracy = num_equal_entries / len(labels)\n",
    "\n",
    "print(accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "[3 4 5 6 7 8]\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "data_red_wine = pd.read_csv('/Users/marlon/VS-Code-Projects/OptimizationNEW/HW1/wine+quality/winequality-red.csv', sep=';')\n",
    "data_white_wine = pd.read_csv('/Users/marlon/VS-Code-Projects/OptimizationNEW/HW1/wine+quality/winequality-red.csv', sep=';')\n",
    "\n",
    "\n",
    "#print(data_red_wine.head())\n",
    "\n",
    "labels_red = np.array(data_red_wine['quality'])\n",
    "labels_white = np.array(data_red_wine['quality'])\n",
    "\n",
    "\n",
    "\n",
    "number_labels_red = np.unique(labels_red)\n",
    "number_labels_white = np.unique(labels_white)\n",
    "\n",
    "\n",
    "print(len(number_labels_red))\n",
    "print(number_labels_white)\n",
    "\n",
    "# In both we only have labels from 3 to 8 (6 classes)\n",
    "\n",
    "# Let us work on the red wine dataset\n",
    "# Data matrix A\n",
    "\n",
    "data_red_wine.drop('quality', axis=1, inplace=True)\n",
    "\n",
    "A = np.array(data_red_wine)\n",
    "\n",
    "print(A.shape[1])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "A_train, A_test, label_train, label_test = train_test_split(A, labels, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.13120699 -0.09788538  0.86302974 -0.11285074  1.11699774  1.11864733]\n",
      " [-0.9391306  -0.68435972 -0.72190713 -0.94986434 -0.76694035  0.28555533]\n",
      " [-0.19296711 -1.16764125  0.4458307   2.36677327  1.52146685  0.37720212]\n",
      " [ 1.1493142   0.01416811 -0.56381939 -0.50460414 -0.54787777  0.34395364]\n",
      " [ 0.15415917 -0.49552297  2.55026878 -0.54687861 -0.96459125 -0.08506572]\n",
      " [ 0.50548578  0.39097948 -1.28606889 -1.11533328  0.56075546 -0.09622113]\n",
      " [-1.4798732  -0.0377065   0.32634976 -0.01176184 -0.20169448 -0.88098601]\n",
      " [-0.95988527  0.98251474 -1.72858814 -0.41602863  0.50107762 -1.34698388]\n",
      " [-0.93614035  1.25116105 -1.76782318 -1.69531192  0.3717296  -0.16214211]\n",
      " [ 0.60241732 -0.95310716 -1.42988627 -0.65515845 -0.43868547  0.7618962 ]\n",
      " [ 1.33017028  1.22144727  0.42106174  0.10088241 -2.40220077 -0.21492056]]\n"
     ]
    }
   ],
   "source": [
    "# Generate initial feature matrix X\n",
    "\n",
    "\n",
    "NUM_LABELS = len(number_labels_red)\n",
    "NUM_FEATURES = A.shape[1]\n",
    "\n",
    "# This is our weight matrix that we initialize like this ; these weights we want to learn\n",
    "# it has 1000 features (rows) with 50 labels each (columns)\n",
    "# X MATRIX\n",
    "weight_matrix = np.random.normal(0, 1, size = (NUM_FEATURES, NUM_LABELS)) # He refers to it as X \n",
    "X = weight_matrix\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
