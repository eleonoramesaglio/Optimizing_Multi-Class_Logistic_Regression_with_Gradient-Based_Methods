{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization for Data Science 2024 Homework 1\n",
    "\n",
    "**Students:**\n",
    "\n",
    "*Alberto Calabrese* - Student ID: 2103405\n",
    "\n",
    "*Greta d'Amore Grelli* - Student ID: 2122424\n",
    "\n",
    "*Eleonora Mesaglio* - Student ID: 2103402\n",
    "\n",
    "*Marlon Helbing* - Student ID: 2106578"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generating the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Set a seed for deterministic outputs\n",
    "SEED = 42\n",
    "np.random.seed(seed = SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A shape:  (1000, 1000)\n",
      "X shape:  (1000, 50)\n",
      "E shape:  (1000, 50)\n",
      "AX+E shape:  (1000, 50)\n"
     ]
    }
   ],
   "source": [
    "NUM_SAMPLES = 1000\n",
    "NUM_FEATURES = 1000\n",
    "NUM_LABELS = 50\n",
    "NUM_EXAMPLES = 1000\n",
    "\n",
    "# A MATRIX\n",
    "# Generate a 1000x1000 matrix with random samples from a standard normal distribution\n",
    "# This is our data matrix, which contains 1000 samples (rows) with 1000 features each (columns)\n",
    "data_matrix = np.random.normal(0, 1, size = (NUM_SAMPLES, NUM_FEATURES))\n",
    "A = data_matrix \n",
    "# 'A' contains random values drawn from N(0,1)\n",
    "print(\"A shape: \", A.shape)\n",
    "\n",
    "# X MATRIX\n",
    "# This is our weight matrix that we initialize like this ; these weights we want to learn\n",
    "# it has 1000 features (rows) with 50 labels each (columns)\n",
    "weight_matrix = np.random.normal(0, 1, size = (NUM_FEATURES, NUM_LABELS))\n",
    "X = weight_matrix\n",
    "# 'X' contains random values drawn from N(0,1)\n",
    "print(\"X shape: \", X.shape)\n",
    "\n",
    "# E MATRIX\n",
    "# This matrix is used to help generating our supervised gold labels \n",
    "# It is of size 1000 training examples (rows) and their labels (columns)\n",
    "generative_matrix = np.random.normal(0, 1, size = (NUM_EXAMPLES, NUM_LABELS))\n",
    "E = generative_matrix\n",
    "# 'E' contains random values drawn from N(0,1)\n",
    "print(\"E shape: \", E.shape)\n",
    "\n",
    "# LABEL VECTOR\n",
    "# Create a vector with numbers from 1 to 50\n",
    "label_vector = np.arange(1, 51)\n",
    "\n",
    "# Now calculate AX+E to generate labels for the 1000 training examples (such that we have a supervised learning set) \n",
    "\n",
    "result_matrix = A @ X + E\n",
    "\n",
    "print(\"AX+E shape: \", result_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MAX INDEX AS CLASS LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels shape:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "# We find our labels by considering the max index in the row as the class label\n",
    "# Find the column indices of maximum values for each row\n",
    "labels = np.argmax(result_matrix, axis=1)\n",
    "print(\"Labels shape: \", labels.shape)\n",
    "\n",
    "# 'labels' now contains the column indices of maximum values for each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = NUM_SAMPLES # samples\n",
    "d = NUM_FEATURES # features\n",
    "k = NUM_LABELS   # labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cost function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(X,A,labels):\n",
    "    # This function is ugly but just for testing if we did smth wrong\n",
    "    sum_1 = 0\n",
    "  \n",
    "    for sample_idx in range(m):\n",
    "\n",
    "        # Take the column at index of the label of the current sample\n",
    "        x_bi = X[:, labels[sample_idx]]\n",
    "        x_bi = -1 * x_bi\n",
    "        a_i = A[sample_idx, :]\n",
    "        sum_1 += (x_bi @ a_i) # automatically calculates (1,1000) x (1000,1)\n",
    "    sum_2 = 0\n",
    "    for sample_idx in range(m):\n",
    "        current = 0 # so we have the correct start value\n",
    "        for label_idx in range(k):\n",
    "            current += np.exp((X[:,label_idx]) @ A[sample_idx,:])\n",
    "        sum_2 += np.log(current)\n",
    "\n",
    "    return sum_1 + sum_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Partial Gradient\n",
    "We define as a block a single column in the parameter matrix $X$. Thus, one block defines all features for a single class. As this is a column vector in the matrix $X$, our partial gradient is now only dependent on $c$.\n",
    "\n",
    "Then, our partial derivatives with respect with one block have the following form\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial f(X)}{\\partial X_{c}} = - A^{T} \\cdot (I_c - Q_c) = A^{T} \\cdot (- I_c + Q_c)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "where \n",
    "\n",
    "$A$ has shape $m \\times d$, it is our given matrix A;\n",
    "\\\n",
    "$I_c$ has shape $m \\times 1$, it is the indicator vector containing $1$'s only at the positions where the label of the current sample $i$ is $c$ and $0$'s everywhere else;\n",
    "$$\n",
    "I_c=\\begin{cases}\n",
    "\t\t\t1 \\quad & \\text{if $label_{i} = c $}\\\\\n",
    "            0 \\quad & \\text{otherwise}\n",
    "\t\t \\end{cases}\n",
    "$$\n",
    "\\\n",
    "$Q_c$ has shape $m \\times 1$, it is the vector calculating the exponential expression $\\dfrac{\\exp(x_{c}^{T}a_{i})}{\\sum_{c' = 1}^{k} \\exp(x_{c'}^{T}a_{i}) }$ for each sample $i$.\n",
    "\n",
    "Thus, our result will be of form $d \\times 1$.\n",
    "\n",
    "Note that the calculations needed for $I_c$ and $Q_c$ only depend on $c$ and $i$. However, as we know all the samples $i$, we construct the vectors $I_c$ and $Q_c$ which are then only dependent on $c$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_gradient(X,A,labels,c):\n",
    "    # We need X as parameter so it changes value across diff gradients\n",
    "\n",
    "    # We define the partial gradient\n",
    "    \n",
    "    # Calculating indicator vector L \n",
    "\n",
    "    # Initialize empty L in size of all samples (=1000)\n",
    "    L = np.zeros((m,1))\n",
    "\n",
    "    # Iterate over labels of each sample\n",
    "    for idx,label in enumerate(labels):\n",
    "        # If there is a label match\n",
    "        if label == c:\n",
    "            # We assign a 1\n",
    "            L[idx] = 1\n",
    "        # If there is no match\n",
    "        else:\n",
    "            # We assign a 0\n",
    "            L[idx] = 0\n",
    "\n",
    "    # Calculating vector Q\n",
    "    # Initialize empty Q in size of all samples (=1000)\n",
    "    \n",
    "    Q = np.zeros((m,1))\n",
    "\n",
    "    # Iterate over all samples\n",
    "    for curr_sample in range(m):\n",
    "    \n",
    "        nominator = np.exp((X[:,c]) @ (A[curr_sample,:]))\n",
    "    \n",
    "        denominator = 0\n",
    "        # Iterate over all labels for the denominator\n",
    "        for curr_label in range(k):\n",
    "            denominator += np.exp((X[:,curr_label]) @ A[curr_sample,:])\n",
    "    \n",
    "        Q[curr_sample] = nominator/denominator\n",
    "\n",
    "    return (np.dot(A.T, ((-1 * L) + Q))).squeeze() # returns (1000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Full gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_gradient(X,A,labels):\n",
    "    # initialize zero gradient of size (d,)\n",
    "    grad = np.zeros(d)\n",
    "    for label in range(k):\n",
    "        grad = np.column_stack((grad, partial_gradient(X,A,labels,label)))\n",
    "\n",
    "    return grad[:,1:] #remove 0 column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X,A,labels,lr, iterations = 20):\n",
    "    X_0 = X\n",
    "    gradient_norms = []\n",
    "    for i in range(iterations): # number of iterations\n",
    "        print(f'Loss at iteration {i} (GD): {cost_function(X_0, A,labels)}')\n",
    "\n",
    "        # Gradient step\n",
    "\n",
    "        # Calculate current gradient\n",
    "        gradient_at_i = full_gradient(X_0,A,labels)\n",
    "        # Calculate the norm\n",
    "        gradient_norms.append(np.linalg.norm(gradient_at_i))\n",
    "        # Gradient step\n",
    "        X_0 = X_0 - lr * gradient_at_i\n",
    "\n",
    "    # Create an array of iteration numbers\n",
    "    iterations = np.arange(len(gradient_norms))\n",
    "\n",
    "    # Create Gradient Norm Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(iterations, gradient_norms, label='Gradient Norm')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Gradient Norm')\n",
    "    plt.title('Gradient Norm across Iterations')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 0 (GD): 93.39358249527868\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gradient_descent(X,A,labels, lr= 0.001,  iterations=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. BCGD Randomized Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BCGD_randomized(X,A,labels, iterations = 500):\n",
    "\n",
    "    X_0 = X\n",
    "    for i in range(iterations): \n",
    "        curr_c = random.randint(0,k-1)\n",
    "        print(\"Current Loss : {}\".format(cost_function(X_0, A, labels)))\n",
    "        # Gradient step\n",
    "        X_0[:, curr_c] = X_0[:, curr_c] - 0.001 * partial_gradient(X_0,A,labels,curr_c) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss : 93.39358249527868\n",
      "Current Loss : 93.1551439503819\n",
      "Current Loss : 91.75785232694761\n",
      "Current Loss : 90.83025853786967\n",
      "Current Loss : 90.3484454300924\n",
      "Current Loss : 89.07651126453129\n",
      "Current Loss : 88.35319848399376\n",
      "Current Loss : 87.44028136588167\n",
      "Current Loss : 86.50551504001487\n",
      "Current Loss : 85.99811663661967\n",
      "Current Loss : 84.5143983419839\n",
      "Current Loss : 83.55935294870869\n",
      "Current Loss : 83.51733101750142\n",
      "Current Loss : 82.98011271347059\n",
      "Current Loss : 82.5558651483152\n",
      "Current Loss : 81.54091778010479\n",
      "Current Loss : 80.9637788352702\n",
      "Current Loss : 80.1990670995583\n",
      "Current Loss : 78.83016910076549\n",
      "Current Loss : 78.24544444480853\n"
     ]
    }
   ],
   "source": [
    "BCGD_randomized(X,A,labels, iterations=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. BCGD Gauss-Southwell Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to calculate the partial gradient of each block\n",
    "# We calculate it in our partial_gradient function.\n",
    "# Thus, we have to iterate over all possible blocks , which in our case are class - many, so k many \n",
    "\n",
    "def Gauss_Southwell(X,A, labels, iterations=20):\n",
    "\n",
    "    learning_rate = 0.01 # TODO : REPLACE BY LIPSCHITZ\n",
    "    X_0 = X\n",
    "    for it in range(iterations): # iterations\n",
    "        \n",
    "        all_partial_gradients = []\n",
    "        for label_idx in range(k): # k is number of all labels\n",
    "            # Calculate the partial gradient of each block\n",
    "            all_partial_gradients.append(partial_gradient(X_0,A,labels,label_idx))\n",
    "\n",
    "        # Gradients will be of size (features,) so in our case (1000,) \n",
    "        all_partial_gradients_norms = [np.linalg.norm(curr_grad) for curr_grad in all_partial_gradients]\n",
    "\n",
    "        max_idx = np.argmax(all_partial_gradients_norms)\n",
    "   \n",
    "        partial_grad = all_partial_gradients[max_idx]\n",
    "\n",
    "        # Gradient Descent \n",
    "        print(f'Loss at iteration {it} (BCGD G.S.): {cost_function(X_0, A,labels)}')\n",
    "        # Gradient step\n",
    "        X_0[:, max_idx] = X_0[:, max_idx] - learning_rate * partial_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 0 (BCGD G.S.): 77.4546820892574\n"
     ]
    }
   ],
   "source": [
    "# Lest's see how it performs\n",
    "Gauss_Southwell(X,A,labels,iterations=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Real Dataset\n",
    "\n",
    "P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. \n",
    "\n",
    "Modeling wine preferences by data mining from physicochemical properties.\n",
    "\n",
    "In Decision Support Systems, Elsevier, 47(4):547-553. ISSN: 0167-9236.\n",
    "\n",
    "Available at: [@Elsevier] http://dx.doi.org/10.1016/j.dss.2009.05.016\n",
    "            [Pre-press (pdf)] http://www3.dsi.uminho.pt/pcortez/winequality09.pdf\n",
    "            [bib] http://www3.dsi.uminho.pt/pcortez/dss09.bib\n",
    "\n",
    "1. Title: Wine Quality \n",
    "\n",
    "2. Sources\n",
    "   \n",
    "   Created by: Paulo Cortez (Univ. Minho), Antonio Cerdeira, Fernando Almeida, Telmo Matos and Jose Reis (CVRVV) @ 2009\n",
    "\n",
    " \n",
    "3. Relevant Information:\n",
    "\n",
    "   The two datasets are related to red and white variants of the Portuguese \"Vinho Verde\" wine.\n",
    "   For more details, consult: http://www.vinhoverde.pt/en/ or the reference [Cortez et al., 2009].\n",
    "   Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables \n",
    "   are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).\n",
    "\n",
    "   These datasets can be viewed as classification or regression tasks.\n",
    "   The classes are ordered and not balanced (e.g. there are munch more normal wines than\n",
    "   excellent or poor ones). Outlier detection algorithms could be used to detect the few excellent\n",
    "   or poor wines. Also, we are not sure if all input variables are relevant. So\n",
    "   it could be interesting to test feature selection methods. \n",
    "\n",
    "4. Number of Instances: red wine - 1599; white wine - 4898. \n",
    "\n",
    "5. Number of Attributes: 11 + output attribute\n",
    "  \n",
    "   Note: several of the attributes may be correlated, thus it makes sense to apply some sort of feature selection.\n",
    "\n",
    "6. Attribute information:\n",
    "\n",
    "   Input variables (based on physicochemical tests):\n",
    "\n",
    "      1 - fixed acidity\n",
    "      \n",
    "      2 - volatile acidity\n",
    "      \n",
    "      3 - citric acid\n",
    "      \n",
    "      4 - residual sugar\n",
    "      \n",
    "      5 - chlorides\n",
    "      \n",
    "      6 - free sulfur dioxide\n",
    "      \n",
    "      7 - total sulfur dioxide\n",
    "      \n",
    "      8 - density\n",
    "      \n",
    "      9 - pH\n",
    "      \n",
    "      10 - sulphates\n",
    "      \n",
    "      11 - alcohol\n",
    "   \n",
    "   Output variable (based on sensory data): \n",
    "   \n",
    "      12 - quality (score between 0 and 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import, Exploration and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4 5 6 7 8]\n",
      "[3 4 5 6 7 8]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Adjust the path to the location of the dataset on your machine -----------------\n",
    "data_red_wine = pd.read_csv('/Users/marlon/VS-Code-Projects/OptimizationNEW/HW1/wine+quality/winequality-red.csv', sep=';')\n",
    "data_white_wine = pd.read_csv('/Users/marlon/VS-Code-Projects/OptimizationNEW/HW1/wine+quality/winequality-white.csv', sep=';')\n",
    "\n",
    "# Let us define the labels from the dataset\n",
    "labels_red = np.array(data_red_wine['quality'])\n",
    "labels_white = np.array(data_red_wine['quality'])\n",
    "\n",
    "# Let us see how many unique labels we have in both datasets\n",
    "number_labels_red = np.unique(labels_red)\n",
    "number_labels_white = np.unique(labels_white)\n",
    "\n",
    "# Print the number of unique labels\n",
    "print(number_labels_red)\n",
    "print(number_labels_white)\n",
    "\n",
    "# In both we only have labels from 3 to 8 (6 classes)\n",
    "\n",
    "# Let us work on the red wine dataset\n",
    "\n",
    "\n",
    "labels = np.array(data_red_wine['quality'])\n",
    "\n",
    "# Set labels so they start from 0 , since our smallest label is 3 and they go up in 1-steps , just subtract 3\n",
    "labels = labels - 3\n",
    "\n",
    "# Drop the quality column\n",
    "data_red_wine.drop('quality', axis=1, inplace=True)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "# Fit and transform the data\n",
    "data_red_wine = pd.DataFrame(scaler.fit_transform(data_red_wine), columns=data_red_wine.columns)\n",
    "\n",
    "# Convert the data to a numpy array\n",
    "A = np.array(data_red_wine)\n",
    "\n",
    "NUM_SAMPLES = data_red_wine.shape[0]\n",
    "NUM_FEATURES = data_red_wine.shape[1]\n",
    "NUM_LABELS = len(number_labels_red)\n",
    "\n",
    "m = NUM_SAMPLES # samples\n",
    "d = NUM_FEATURES # features\n",
    "k = NUM_LABELS   # labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1279,)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "A_train, A_test, label_train, label_test = train_test_split(A, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "A_train.shape\n",
    "label_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate Weight Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 6)\n"
     ]
    }
   ],
   "source": [
    "weight_matrix = np.random.normal(0, 1, size = (NUM_FEATURES, NUM_LABELS))\n",
    "X = weight_matrix\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1279\n",
      "Current Loss : 2053.9586771960003\n",
      "1279\n",
      "Current Loss : 2050.9296191687213\n",
      "1279\n",
      "Current Loss : 2046.8469388858857\n",
      "1279\n",
      "Current Loss : 2043.2985785254198\n",
      "1279\n",
      "Current Loss : 2041.0989213486255\n",
      "1279\n",
      "Current Loss : 2034.3520097606556\n",
      "1279\n",
      "Current Loss : 2032.6089685740878\n",
      "1279\n",
      "Current Loss : 2029.7079038834381\n",
      "1279\n",
      "Current Loss : 2028.208026414594\n",
      "1279\n",
      "Current Loss : 2024.8426015955008\n",
      "1279\n",
      "Current Loss : 2014.9247899027116\n",
      "1279\n",
      "Current Loss : 2012.548137564283\n",
      "1279\n",
      "Current Loss : 2011.3417801483918\n",
      "1279\n",
      "Current Loss : 2005.7427338143646\n",
      "1279\n",
      "Current Loss : 2001.1291731172792\n",
      "1279\n",
      "Current Loss : 1998.9376437168278\n",
      "1279\n",
      "Current Loss : 1997.8934694437628\n",
      "1279\n",
      "Current Loss : 1993.9751422487795\n",
      "1279\n",
      "Current Loss : 1993.0815260026195\n",
      "1279\n",
      "Current Loss : 1986.4335915064803\n",
      "1279\n",
      "Current Loss : 1981.4863958613414\n",
      "1279\n",
      "Current Loss : 1980.8004202710933\n",
      "1279\n",
      "Current Loss : 1979.0030554584173\n",
      "1279\n",
      "Current Loss : 1978.3698543462904\n",
      "1279\n",
      "Current Loss : 1975.1490500844843\n",
      "1279\n",
      "Current Loss : 1973.5435928374543\n",
      "1279\n",
      "Current Loss : 1972.1123149137936\n",
      "1279\n",
      "Current Loss : 1970.8193150058062\n",
      "1279\n",
      "Current Loss : 1968.3363559474392\n",
      "1279\n",
      "Current Loss : 1967.7074819868972\n",
      "1279\n",
      "Current Loss : 1964.623280599336\n",
      "1279\n",
      "Current Loss : 1961.2563959678398\n",
      "1279\n",
      "Current Loss : 1959.0807801686733\n",
      "1279\n",
      "Current Loss : 1956.4484493788927\n",
      "1279\n",
      "Current Loss : 1954.573691850922\n",
      "1279\n",
      "Current Loss : 1952.4453581735227\n",
      "1279\n",
      "Current Loss : 1950.652231237252\n",
      "1279\n",
      "Current Loss : 1949.1039159251313\n",
      "1279\n",
      "Current Loss : 1948.5471622206712\n",
      "1279\n",
      "Current Loss : 1945.9119844250195\n",
      "1279\n",
      "Current Loss : 1944.5465053865826\n",
      "1279\n",
      "Current Loss : 1943.3274361598942\n",
      "1279\n",
      "Current Loss : 1942.2287297401035\n",
      "1279\n",
      "Current Loss : 1941.2317575661907\n",
      "1279\n",
      "Current Loss : 1938.5243360531397\n",
      "1279\n",
      "Current Loss : 1938.0558472477337\n",
      "1279\n",
      "Current Loss : 1935.5937983602037\n",
      "1279\n",
      "Current Loss : 1933.3279783575579\n",
      "1279\n",
      "Current Loss : 1931.772965722446\n",
      "1279\n",
      "Current Loss : 1930.874995199768\n",
      "1279\n",
      "Current Loss : 1930.0642914403907\n",
      "1279\n",
      "Current Loss : 1928.9630624760969\n",
      "1279\n",
      "Current Loss : 1926.658537222651\n",
      "1279\n",
      "Current Loss : 1925.6549913291858\n",
      "1279\n",
      "Current Loss : 1925.2222055387954\n",
      "1279\n",
      "Current Loss : 1924.8304367222233\n",
      "1279\n",
      "Current Loss : 1923.4511840200494\n",
      "1279\n",
      "Current Loss : 1923.0823850224426\n",
      "1279\n",
      "Current Loss : 1922.745504040518\n",
      "1279\n",
      "Current Loss : 1922.0372596158438\n",
      "1279\n",
      "Current Loss : 1921.7350930701116\n",
      "1279\n",
      "Current Loss : 1921.0960987458552\n",
      "1279\n",
      "Current Loss : 1919.1305272059917\n",
      "1279\n",
      "Current Loss : 1917.0066131561475\n",
      "1279\n",
      "Current Loss : 1915.7750097715868\n",
      "1279\n",
      "Current Loss : 1914.8423427635382\n",
      "1279\n",
      "Current Loss : 1912.878157165017\n",
      "1279\n",
      "Current Loss : 1911.0510984296825\n",
      "1279\n",
      "Current Loss : 1910.7655251891213\n",
      "1279\n",
      "Current Loss : 1909.1161370885184\n",
      "1279\n",
      "Current Loss : 1908.266245678282\n",
      "1279\n",
      "Current Loss : 1907.4783228916965\n",
      "1279\n",
      "Current Loss : 1906.7440058173877\n",
      "1279\n",
      "Current Loss : 1906.4814190702236\n",
      "1279\n",
      "Current Loss : 1905.8824039557874\n",
      "1279\n",
      "Current Loss : 1905.1952167541635\n",
      "1279\n",
      "Current Loss : 1904.5502518346123\n",
      "1279\n",
      "Current Loss : 1904.0144771950563\n",
      "1279\n",
      "Current Loss : 1903.526858028591\n",
      "1279\n",
      "Current Loss : 1902.9211442793542\n",
      "1279\n",
      "Current Loss : 1901.2328815864375\n",
      "1279\n",
      "Current Loss : 1900.6659216291023\n",
      "1279\n",
      "Current Loss : 1900.2194176341093\n",
      "1279\n",
      "Current Loss : 1899.8095915419387\n",
      "1279\n",
      "Current Loss : 1898.2385794203465\n",
      "1279\n",
      "Current Loss : 1896.78181129908\n",
      "1279\n",
      "Current Loss : 1895.3162713671636\n",
      "1279\n",
      "Current Loss : 1894.2194006910463\n",
      "1279\n",
      "Current Loss : 1892.8444474968721\n",
      "1279\n",
      "Current Loss : 1892.612681629054\n",
      "1279\n",
      "Current Loss : 1892.2327647054915\n",
      "1279\n",
      "Current Loss : 1890.9388456571764\n",
      "1279\n",
      "Current Loss : 1890.4185118854357\n",
      "1279\n",
      "Current Loss : 1889.412008742165\n",
      "1279\n",
      "Current Loss : 1888.4883505161247\n",
      "1279\n",
      "Current Loss : 1887.2798776508118\n",
      "1279\n",
      "Current Loss : 1886.7878529447235\n",
      "1279\n",
      "Current Loss : 1886.5692310643506\n",
      "1279\n",
      "Current Loss : 1885.7268462392958\n",
      "1279\n",
      "Current Loss : 1884.5164721991814\n",
      "1279\n",
      "Current Loss : 1883.736583733929\n",
      "1279\n",
      "Current Loss : 1883.3993947662314\n",
      "1279\n",
      "Current Loss : 1883.094094713532\n",
      "1279\n",
      "Current Loss : 1882.0508178897815\n",
      "1279\n",
      "Current Loss : 1881.3433333211954\n",
      "1279\n",
      "Current Loss : 1881.1392641391321\n",
      "1279\n",
      "Current Loss : 1880.8584826932788\n",
      "1279\n",
      "Current Loss : 1880.6009114738572\n",
      "1279\n",
      "Current Loss : 1880.3629664630737\n",
      "1279\n",
      "Current Loss : 1880.1843314978123\n",
      "1279\n",
      "Current Loss : 1879.0437941854118\n",
      "1279\n",
      "Current Loss : 1878.8791109372373\n",
      "1279\n",
      "Current Loss : 1878.415099089351\n",
      "1279\n",
      "Current Loss : 1877.495428836456\n",
      "1279\n",
      "Current Loss : 1876.6783536492057\n",
      "1279\n",
      "Current Loss : 1876.529325632546\n",
      "1279\n",
      "Current Loss : 1876.3917393279542\n",
      "1279\n",
      "Current Loss : 1876.2640640555205\n",
      "1279\n",
      "Current Loss : 1876.1451393848058\n",
      "1279\n",
      "Current Loss : 1875.7060364788003\n",
      "1279\n",
      "Current Loss : 1875.486039126423\n",
      "1279\n",
      "Current Loss : 1875.2856843987286\n",
      "1279\n",
      "Current Loss : 1874.8693494358617\n",
      "1279\n",
      "Current Loss : 1874.760239157934\n",
      "1279\n",
      "Current Loss : 1874.6583879818338\n",
      "1279\n",
      "Current Loss : 1874.475960597688\n",
      "1279\n",
      "Current Loss : 1874.3069246678806\n",
      "1279\n",
      "Current Loss : 1873.9102071695424\n",
      "1279\n",
      "Current Loss : 1873.1676295828554\n",
      "1279\n",
      "Current Loss : 1873.0753606778903\n",
      "1279\n",
      "Current Loss : 1872.0125808004777\n",
      "1279\n",
      "Current Loss : 1871.9255170891886\n",
      "1279\n",
      "Current Loss : 1871.7660416285185\n",
      "1279\n",
      "Current Loss : 1871.6853779350208\n",
      "1279\n",
      "Current Loss : 1871.0443015721573\n",
      "1279\n",
      "Current Loss : 1870.9675822318973\n",
      "1279\n",
      "Current Loss : 1870.8237598642972\n",
      "1279\n",
      "Current Loss : 1870.228670855987\n",
      "1279\n",
      "Current Loss : 1869.8505252622522\n",
      "1279\n",
      "Current Loss : 1868.8594703996202\n",
      "1279\n",
      "Current Loss : 1868.7862785838092\n",
      "1279\n",
      "Current Loss : 1868.4286691617356\n",
      "1279\n",
      "Current Loss : 1868.3594324464539\n",
      "1279\n",
      "Current Loss : 1868.2276070352384\n",
      "1279\n",
      "Current Loss : 1867.5653058285461\n",
      "1279\n",
      "Current Loss : 1866.967039415432\n",
      "1279\n",
      "Current Loss : 1866.9036042119587\n",
      "1279\n",
      "Current Loss : 1866.7762775606946\n",
      "1279\n",
      "Current Loss : 1866.6586529124681\n",
      "1279\n",
      "Current Loss : 1866.3135846173918\n",
      "1279\n",
      "Current Loss : 1866.2031810270241\n",
      "1279\n",
      "Current Loss : 1865.8740255599298\n",
      "1279\n",
      "Current Loss : 1865.333867208178\n",
      "1279\n",
      "Current Loss : 1865.2741010670352\n",
      "1279\n",
      "Current Loss : 1865.2181517675265\n",
      "1279\n",
      "Current Loss : 1865.1159011466375\n",
      "1279\n",
      "Current Loss : 1864.614354267631\n",
      "1279\n",
      "Current Loss : 1863.695680210977\n",
      "1279\n",
      "Current Loss : 1862.8369710707027\n",
      "1279\n",
      "Current Loss : 1862.031212714741\n",
      "1279\n",
      "Current Loss : 1861.936760698849\n",
      "1279\n",
      "Current Loss : 1861.1789644149428\n",
      "1279\n",
      "Current Loss : 1861.1244453927034\n",
      "1279\n",
      "Current Loss : 1861.0368465468578\n",
      "1279\n",
      "Current Loss : 1860.5771954586933\n",
      "1279\n",
      "Current Loss : 1860.5261274492643\n",
      "1279\n",
      "Current Loss : 1860.4782694204496\n",
      "1279\n",
      "Current Loss : 1860.3979723314583\n",
      "1279\n",
      "Current Loss : 1859.8520103652752\n",
      "1279\n",
      "Current Loss : 1859.54151350388\n",
      "1279\n",
      "Current Loss : 1859.1191116524715\n",
      "1279\n",
      "Current Loss : 1858.6226642876284\n",
      "1279\n",
      "Current Loss : 1857.91390929953\n",
      "1279\n",
      "Current Loss : 1857.2471941971742\n",
      "1279\n",
      "Current Loss : 1856.6191905164687\n",
      "1279\n",
      "Current Loss : 1856.5730537004722\n",
      "1279\n",
      "Current Loss : 1855.9806602061258\n",
      "1279\n",
      "Current Loss : 1855.5299585300993\n",
      "1279\n",
      "Current Loss : 1855.449215238822\n",
      "1279\n",
      "Current Loss : 1855.0674179986552\n",
      "1279\n",
      "Current Loss : 1854.7758781564955\n",
      "1279\n",
      "Current Loss : 1854.3582704962694\n",
      "1279\n",
      "Current Loss : 1853.8035568326793\n",
      "1279\n",
      "Current Loss : 1853.759280608732\n",
      "1279\n",
      "Current Loss : 1853.684736214665\n",
      "1279\n",
      "Current Loss : 1853.1607999533855\n",
      "1279\n",
      "Current Loss : 1852.8137597374175\n",
      "1279\n",
      "Current Loss : 1852.493849746839\n",
      "1279\n",
      "Current Loss : 1852.4519321622413\n",
      "1279\n",
      "Current Loss : 1852.3855758289644\n",
      "1279\n",
      "Current Loss : 1852.324184464689\n",
      "1279\n",
      "Current Loss : 1852.031463786931\n",
      "1279\n",
      "Current Loss : 1851.538732603199\n",
      "1279\n",
      "Current Loss : 1851.4818253455999\n",
      "1279\n",
      "Current Loss : 1851.4282510514815\n",
      "1279\n",
      "Current Loss : 1851.3886994825682\n",
      "1279\n",
      "Current Loss : 1851.1216833736048\n",
      "1279\n",
      "Current Loss : 1851.0848828855017\n",
      "1279\n",
      "Current Loss : 1850.6199093619546\n",
      "1279\n",
      "Current Loss : 1850.3463979154028\n",
      "1279\n",
      "Current Loss : 1850.3112641482012\n",
      "1279\n",
      "Current Loss : 1850.0677612069853\n",
      "1279\n",
      "Current Loss : 1849.6875928508432\n",
      "1279\n",
      "Current Loss : 1849.4269723920534\n",
      "1279\n",
      "Current Loss : 1849.1776164927214\n",
      "1279\n",
      "Current Loss : 1848.74361923031\n",
      "1279\n",
      "Current Loss : 1848.6929937145692\n",
      "1279\n",
      "Current Loss : 1848.659284057585\n",
      "1279\n",
      "Current Loss : 1848.440270569456\n",
      "1279\n",
      "Current Loss : 1848.4085284255912\n",
      "1279\n",
      "Current Loss : 1848.3617910236276\n",
      "1279\n",
      "Current Loss : 1848.007831576749\n",
      "1279\n",
      "Current Loss : 1847.963264105724\n",
      "1279\n",
      "Current Loss : 1847.630876954065\n",
      "1279\n",
      "Current Loss : 1847.3177898974207\n",
      "1279\n",
      "Current Loss : 1847.02177539403\n",
      "1279\n",
      "Current Loss : 1846.8258450262263\n",
      "1279\n",
      "Current Loss : 1846.6468010055326\n",
      "1279\n",
      "Current Loss : 1846.617927179468\n",
      "1279\n",
      "Current Loss : 1846.575429381138\n",
      "1279\n",
      "Current Loss : 1846.5482367456968\n",
      "1279\n",
      "Current Loss : 1846.5079263107527\n",
      "1279\n",
      "Current Loss : 1846.1043111501895\n",
      "1279\n",
      "Current Loss : 1845.9423475778428\n",
      "1279\n",
      "Current Loss : 1845.794284332693\n",
      "1279\n",
      "Current Loss : 1845.4151423881062\n",
      "1279\n",
      "Current Loss : 1845.3890407142012\n",
      "1279\n",
      "Current Loss : 1845.1526195686124\n",
      "1279\n",
      "Current Loss : 1845.0182787992198\n",
      "1279\n",
      "Current Loss : 1844.9811292104143\n",
      "1279\n",
      "Current Loss : 1844.624850629541\n",
      "1279\n",
      "Current Loss : 1844.3992923660637\n",
      "1279\n",
      "Current Loss : 1844.1235358058243\n",
      "1279\n",
      "Current Loss : 1844.0879702307143\n",
      "1279\n",
      "Current Loss : 1844.0539983456556\n",
      "1279\n",
      "Current Loss : 1844.029126438591\n",
      "1279\n",
      "Current Loss : 1843.9966009704558\n",
      "1279\n",
      "Current Loss : 1843.8760246471472\n",
      "1279\n",
      "Current Loss : 1843.8450019744437\n",
      "1279\n",
      "Current Loss : 1843.7345278097569\n",
      "1279\n",
      "Current Loss : 1843.704793932757\n",
      "1279\n",
      "Current Loss : 1843.3723609062629\n",
      "1279\n",
      "Current Loss : 1843.110949174808\n",
      "1279\n",
      "Current Loss : 1842.894960137508\n",
      "1279\n",
      "Current Loss : 1842.8716074217627\n",
      "1279\n",
      "Current Loss : 1842.771121132358\n",
      "1279\n",
      "Current Loss : 1842.6784204817986\n",
      "1279\n",
      "Current Loss : 1842.3672696397005\n",
      "1279\n",
      "Current Loss : 1842.0734277101767\n",
      "1279\n",
      "Current Loss : 1842.0455747773185\n",
      "1279\n",
      "Current Loss : 1842.0231335370836\n",
      "1279\n",
      "Current Loss : 1841.817003497861\n",
      "1279\n",
      "Current Loss : 1841.7901933185383\n",
      "1279\n",
      "Current Loss : 1841.5134545443375\n",
      "1279\n",
      "Current Loss : 1841.4877071136377\n",
      "1279\n",
      "Current Loss : 1841.4039178372811\n",
      "1279\n",
      "Current Loss : 1841.3823679933073\n",
      "1279\n",
      "Current Loss : 1841.1372635430516\n",
      "1279\n",
      "Current Loss : 1841.0595192891765\n",
      "1279\n",
      "Current Loss : 1840.8616854331356\n",
      "1279\n",
      "Current Loss : 1840.6024652462952\n",
      "1279\n",
      "Current Loss : 1840.5306096218478\n",
      "1279\n",
      "Current Loss : 1840.4634231735608\n",
      "1279\n",
      "Current Loss : 1840.4391958772953\n",
      "1279\n",
      "Current Loss : 1840.4158509605854\n",
      "1279\n",
      "Current Loss : 1840.2255453757355\n",
      "1279\n",
      "Current Loss : 1839.9927943760763\n",
      "1279\n",
      "Current Loss : 1839.808654460829\n",
      "1279\n",
      "Current Loss : 1839.7462024927972\n",
      "1279\n",
      "Current Loss : 1839.7235171751743\n",
      "1279\n",
      "Current Loss : 1839.6648439259388\n",
      "1279\n",
      "Current Loss : 1839.4420704676127\n",
      "1279\n",
      "Current Loss : 1839.2284870603971\n",
      "1279\n",
      "Current Loss : 1839.2087043403046\n",
      "1279\n",
      "Current Loss : 1839.1529942787374\n",
      "1279\n",
      "Current Loss : 1839.134148704284\n",
      "1279\n",
      "Current Loss : 1838.9564681299296\n",
      "1279\n",
      "Current Loss : 1838.717911220999\n",
      "1279\n",
      "Current Loss : 1838.6996714015988\n",
      "1279\n",
      "Current Loss : 1838.527877385693\n",
      "1279\n",
      "Current Loss : 1838.5103022202975\n",
      "1279\n",
      "Current Loss : 1838.2852326058069\n",
      "1279\n",
      "Current Loss : 1838.0723931365856\n",
      "1279\n",
      "Current Loss : 1837.906734800462\n",
      "1279\n",
      "Current Loss : 1837.8551356429848\n",
      "1279\n",
      "Current Loss : 1837.6943149361155\n",
      "1279\n",
      "Current Loss : 1837.6770028283079\n",
      "1279\n",
      "Current Loss : 1837.476943513759\n",
      "1279\n",
      "Current Loss : 1837.460574555401\n",
      "1279\n",
      "Current Loss : 1837.4115135630136\n",
      "1279\n",
      "Current Loss : 1837.3957834853197\n",
      "1279\n",
      "Current Loss : 1837.374418209622\n",
      "1279\n",
      "Current Loss : 1837.3277274101902\n",
      "1279\n",
      "Current Loss : 1837.1287208535261\n",
      "1279\n",
      "Current Loss : 1836.973195574044\n",
      "1279\n",
      "Current Loss : 1836.9288711408362\n",
      "1279\n",
      "Current Loss : 1836.8863720644883\n",
      "1279\n",
      "Current Loss : 1836.6951679341948\n",
      "1279\n",
      "Current Loss : 1836.5113609457271\n",
      "1279\n",
      "Current Loss : 1836.4965119256326\n",
      "1279\n",
      "Current Loss : 1836.3461756112783\n",
      "1279\n",
      "Current Loss : 1836.1699167798308\n",
      "1279\n",
      "Current Loss : 1836.0002604049453\n",
      "1279\n",
      "Current Loss : 1835.854664424905\n",
      "1279\n",
      "Current Loss : 1835.8341606908616\n",
      "1279\n",
      "Current Loss : 1835.8201219333357\n",
      "1279\n",
      "Current Loss : 1835.77883253669\n",
      "1279\n",
      "Current Loss : 1835.5956592363793\n",
      "1279\n",
      "Current Loss : 1835.5560489777627\n",
      "1279\n",
      "Current Loss : 1835.4150118146322\n",
      "1279\n",
      "Current Loss : 1835.2422734199367\n",
      "1279\n",
      "Current Loss : 1835.1054624029864\n",
      "1279\n",
      "Current Loss : 1835.0859034689347\n",
      "1279\n",
      "Current Loss : 1835.0481884085493\n",
      "1279\n",
      "Current Loss : 1834.8867246933073\n",
      "1279\n",
      "Current Loss : 1834.8678661727117\n",
      "1279\n",
      "Current Loss : 1834.7348968307874\n",
      "1279\n",
      "Current Loss : 1834.6053532030348\n",
      "1279\n",
      "Current Loss : 1834.5868048442383\n",
      "1279\n",
      "Current Loss : 1834.5687389608731\n",
      "1279\n",
      "Current Loss : 1834.4079749557275\n",
      "1279\n",
      "Current Loss : 1834.39408636299\n",
      "1279\n",
      "Current Loss : 1834.3807255458553\n",
      "1279\n",
      "Current Loss : 1834.2282856822378\n",
      "1279\n",
      "Current Loss : 1834.2153255535961\n",
      "1279\n",
      "Current Loss : 1834.089448644356\n",
      "1279\n",
      "Current Loss : 1834.0768647257805\n",
      "1279\n",
      "Current Loss : 1833.9540475066403\n",
      "1279\n",
      "Current Loss : 1833.8020029674624\n",
      "1279\n",
      "Current Loss : 1833.7844652206416\n",
      "1279\n",
      "Current Loss : 1833.6649342796368\n",
      "1279\n",
      "Current Loss : 1833.652802801219\n",
      "1279\n",
      "Current Loss : 1833.6356113576019\n",
      "1279\n",
      "Current Loss : 1833.6001630166236\n",
      "1279\n",
      "Current Loss : 1833.4540453637355\n",
      "1279\n",
      "Current Loss : 1833.313233995892\n",
      "1279\n",
      "Current Loss : 1833.2965855279347\n",
      "1279\n",
      "Current Loss : 1833.160848191308\n",
      "1279\n",
      "Current Loss : 1833.149660191303\n",
      "1279\n",
      "Current Loss : 1833.1388523541852\n",
      "1279\n",
      "Current Loss : 1832.9980563957258\n",
      "1279\n",
      "Current Loss : 1832.9875389570936\n",
      "1279\n",
      "Current Loss : 1832.8540400720615\n",
      "1279\n",
      "Current Loss : 1832.727479379887\n",
      "1279\n",
      "Current Loss : 1832.6125715949597\n",
      "1279\n",
      "Current Loss : 1832.5004925505614\n",
      "1279\n",
      "Current Loss : 1832.4900225271695\n",
      "1279\n",
      "Current Loss : 1832.370600930589\n",
      "1279\n",
      "Current Loss : 1832.261498234628\n",
      "1279\n",
      "Current Loss : 1832.1485112311655\n",
      "1279\n",
      "Current Loss : 1832.041281586794\n",
      "1279\n",
      "Current Loss : 1831.9394816704132\n",
      "1279\n",
      "Current Loss : 1831.8428041093414\n",
      "1279\n",
      "Current Loss : 1831.809176993628\n",
      "1279\n",
      "Current Loss : 1831.776527168482\n",
      "1279\n",
      "Current Loss : 1831.6714484270678\n",
      "1279\n",
      "Current Loss : 1831.546106601457\n",
      "1279\n",
      "Current Loss : 1831.45505224406\n",
      "1279\n",
      "Current Loss : 1831.3530872925996\n",
      "1279\n",
      "Current Loss : 1831.2535300806537\n",
      "1279\n",
      "Current Loss : 1831.1563036663779\n",
      "1279\n",
      "Current Loss : 1831.0368495554214\n",
      "1279\n",
      "Current Loss : 1830.9215826660663\n",
      "1279\n",
      "Current Loss : 1830.8272869845744\n",
      "1279\n",
      "Current Loss : 1830.8167955127878\n",
      "1279\n",
      "Current Loss : 1830.7060104207887\n",
      "1279\n",
      "Current Loss : 1830.621506349729\n",
      "1279\n",
      "Current Loss : 1830.6114353989753\n",
      "1279\n",
      "Current Loss : 1830.5950512315087\n",
      "1279\n",
      "Current Loss : 1830.5792141466643\n",
      "1279\n",
      "Current Loss : 1830.5638419671595\n",
      "1279\n",
      "Current Loss : 1830.5488782678376\n",
      "1279\n",
      "Current Loss : 1830.534282975086\n",
      "1279\n",
      "Current Loss : 1830.5243810044315\n",
      "1279\n",
      "Current Loss : 1830.5147993476598\n",
      "1279\n",
      "Current Loss : 1830.5004914563078\n",
      "1279\n",
      "Current Loss : 1830.469565701972\n",
      "1279\n",
      "Current Loss : 1830.3636226347348\n",
      "1279\n",
      "Current Loss : 1830.2613088784806\n",
      "1279\n",
      "Current Loss : 1830.1697405403609\n",
      "1279\n",
      "Current Loss : 1830.0900034636547\n",
      "1279\n",
      "Current Loss : 1829.9917901066417\n",
      "1279\n",
      "Current Loss : 1829.9779715442428\n",
      "1279\n",
      "Current Loss : 1829.8889821518671\n",
      "1279\n",
      "Current Loss : 1829.8587765350426\n",
      "1279\n",
      "Current Loss : 1829.7640451092702\n",
      "1279\n",
      "Current Loss : 1829.75057803543\n",
      "1279\n",
      "Current Loss : 1829.7210638218755\n",
      "1279\n",
      "Current Loss : 1829.6293957150594\n",
      "1279\n",
      "Current Loss : 1829.540789001997\n",
      "1279\n",
      "Current Loss : 1829.5277672048242\n",
      "1279\n",
      "Current Loss : 1829.441744347825\n",
      "1279\n",
      "Current Loss : 1829.4330048173272\n",
      "1279\n",
      "Current Loss : 1829.4041132570123\n",
      "1279\n",
      "Current Loss : 1829.3301689620134\n",
      "1279\n",
      "Current Loss : 1829.2598407846388\n",
      "1279\n",
      "Current Loss : 1829.1763506850773\n",
      "1279\n",
      "Current Loss : 1829.148378641724\n",
      "1279\n",
      "Current Loss : 1829.0669498905647\n",
      "1279\n",
      "Current Loss : 1829.0582984768046\n",
      "1279\n",
      "Current Loss : 1828.9917761737302\n",
      "1279\n",
      "Current Loss : 1828.9789390847889\n",
      "1279\n",
      "Current Loss : 1828.915618399824\n",
      "1279\n",
      "Current Loss : 1828.8553242781024\n",
      "1279\n",
      "Current Loss : 1828.7762051424922\n",
      "1279\n",
      "Current Loss : 1828.7635598816155\n",
      "1279\n",
      "Current Loss : 1828.751194314085\n",
      "1279\n",
      "Current Loss : 1828.6680640133095\n",
      "1279\n",
      "Current Loss : 1828.6594706640228\n",
      "1279\n",
      "Current Loss : 1828.6023699708317\n",
      "1279\n",
      "Current Loss : 1828.5939839210632\n",
      "1279\n",
      "Current Loss : 1828.581860775725\n",
      "1279\n",
      "Current Loss : 1828.504663382952\n",
      "1279\n",
      "Current Loss : 1828.4964421569757\n",
      "1279\n",
      "Current Loss : 1828.4844740189105\n",
      "1279\n",
      "Current Loss : 1828.4727541380487\n",
      "1279\n",
      "Current Loss : 1828.3971067094415\n",
      "1279\n",
      "Current Loss : 1828.3855470036624\n",
      "1279\n",
      "Current Loss : 1828.331322794113\n",
      "1279\n",
      "Current Loss : 1828.319986282425\n",
      "1279\n",
      "Current Loss : 1828.3088727424781\n",
      "1279\n",
      "Current Loss : 1828.2571926367318\n",
      "1279\n",
      "Current Loss : 1828.1833442568454\n",
      "1279\n",
      "Current Loss : 1828.1749810008246\n",
      "1279\n",
      "Current Loss : 1828.1258103789205\n",
      "1279\n",
      "Current Loss : 1828.053706644539\n",
      "1279\n",
      "Current Loss : 1828.0279711042556\n",
      "1279\n",
      "Current Loss : 1827.9575992420832\n",
      "1279\n",
      "Current Loss : 1827.8887915760713\n",
      "1279\n",
      "Current Loss : 1827.863879689698\n",
      "1279\n",
      "Current Loss : 1827.8555992011063\n",
      "1279\n",
      "Current Loss : 1827.8475578588836\n",
      "1279\n",
      "Current Loss : 1827.8232433950143\n",
      "1279\n",
      "Current Loss : 1827.7560920229985\n",
      "1279\n",
      "Current Loss : 1827.7324149808546\n",
      "1279\n",
      "Current Loss : 1827.6860202655253\n",
      "1279\n",
      "Current Loss : 1827.6748897864238\n",
      "1279\n",
      "Current Loss : 1827.6669797700931\n",
      "1279\n",
      "Current Loss : 1827.6560432763947\n",
      "1279\n",
      "Current Loss : 1827.6483157142438\n",
      "1279\n",
      "Current Loss : 1827.640800439155\n",
      "1279\n",
      "Current Loss : 1827.5963494177488\n",
      "1279\n",
      "Current Loss : 1827.588987807635\n",
      "1279\n",
      "Current Loss : 1827.578136071174\n",
      "1279\n",
      "Current Loss : 1827.5356488540117\n",
      "1279\n",
      "Current Loss : 1827.4950980967985\n",
      "1279\n",
      "Current Loss : 1827.4563792180645\n",
      "1279\n",
      "Current Loss : 1827.449030970018\n",
      "1279\n",
      "Current Loss : 1827.3737332500386\n",
      "1279\n",
      "Current Loss : 1827.3507632897351\n",
      "1279\n",
      "Current Loss : 1827.3401283114897\n",
      "1279\n",
      "Current Loss : 1827.303237635833\n",
      "1279\n",
      "Current Loss : 1827.2382760019264\n",
      "1279\n",
      "Current Loss : 1827.1658737106109\n",
      "1279\n",
      "Current Loss : 1827.1554480941825\n",
      "1279\n",
      "Current Loss : 1827.1452506356477\n",
      "1279\n",
      "Current Loss : 1827.1381101122308\n",
      "1279\n",
      "Current Loss : 1827.0683114575306\n",
      "1279\n",
      "Current Loss : 1827.0007841222332\n",
      "1279\n",
      "Current Loss : 1826.9660276476754\n",
      "1279\n",
      "Current Loss : 1826.9032255123816\n",
      "1279\n",
      "Current Loss : 1826.8382883505337\n",
      "1279\n",
      "Current Loss : 1826.7754386739591\n",
      "1279\n",
      "Current Loss : 1826.714596527831\n",
      "1279\n",
      "Current Loss : 1826.6539015341243\n",
      "1279\n",
      "Current Loss : 1826.594557801974\n",
      "1279\n",
      "Current Loss : 1826.5365284196816\n",
      "1279\n",
      "Current Loss : 1826.514099491242\n",
      "1279\n",
      "Current Loss : 1826.4921203984395\n",
      "1279\n",
      "Current Loss : 1826.4355893282932\n",
      "1279\n",
      "Current Loss : 1826.428900770417\n",
      "1279\n",
      "Current Loss : 1826.4223877197267\n",
      "1279\n",
      "Current Loss : 1826.4123617860769\n",
      "1279\n",
      "Current Loss : 1826.4059779131267\n",
      "1279\n",
      "Current Loss : 1826.3846660992178\n",
      "1279\n",
      "Current Loss : 1826.3784663263123\n",
      "1279\n",
      "Current Loss : 1826.323051024515\n",
      "1279\n",
      "Current Loss : 1826.2907581520012\n",
      "1279\n",
      "Current Loss : 1826.2846358316417\n",
      "1279\n",
      "Current Loss : 1826.2638871066074\n",
      "1279\n",
      "Current Loss : 1826.2539843295083\n",
      "1279\n",
      "Current Loss : 1826.2230470066231\n",
      "1279\n",
      "Current Loss : 1826.217027479922\n",
      "1279\n",
      "Current Loss : 1826.207270540915\n",
      "1279\n",
      "Current Loss : 1826.149991017133\n",
      "1279\n",
      "Current Loss : 1826.1296593252337\n",
      "1279\n",
      "Current Loss : 1826.1097095187697\n",
      "1279\n",
      "Current Loss : 1826.1039245820427\n",
      "1279\n",
      "Current Loss : 1826.0982815999585\n",
      "1279\n",
      "Current Loss : 1826.0445315714517\n",
      "1279\n",
      "Current Loss : 1826.015039508553\n",
      "1279\n",
      "Current Loss : 1825.9598822400194\n",
      "1279\n",
      "Current Loss : 1825.9064751649732\n",
      "1279\n",
      "Current Loss : 1825.8868318589498\n",
      "1279\n",
      "Current Loss : 1825.8814246395307\n",
      "1279\n",
      "Current Loss : 1825.8294385942438\n",
      "1279\n",
      "Current Loss : 1825.7785990478299\n",
      "1279\n",
      "Current Loss : 1825.7595372570606\n",
      "1279\n",
      "Current Loss : 1825.7080953664206\n",
      "1279\n",
      "Current Loss : 1825.6986617584932\n",
      "1279\n",
      "Current Loss : 1825.6488926849938\n",
      "1279\n",
      "Current Loss : 1825.6006821084895\n",
      "1279\n",
      "Current Loss : 1825.5731800739545\n",
      "1279\n",
      "Current Loss : 1825.5265752264177\n",
      "1279\n",
      "Current Loss : 1825.500342301504\n",
      "1279\n",
      "Current Loss : 1825.4912223587514\n",
      "1279\n",
      "Current Loss : 1825.4860376153742\n",
      "1279\n",
      "Current Loss : 1825.4670421283872\n",
      "1279\n",
      "Current Loss : 1825.4418643256727\n",
      "1279\n",
      "Current Loss : 1825.3932435950248\n",
      "1279\n",
      "Current Loss : 1825.3692045641474\n",
      "1279\n",
      "Current Loss : 1825.350632665886\n",
      "1279\n",
      "Current Loss : 1825.3455305002014\n",
      "1279\n",
      "Current Loss : 1825.3365376579618\n",
      "1279\n",
      "Current Loss : 1825.318364284264\n",
      "1279\n",
      "Current Loss : 1825.271014249378\n",
      "1279\n",
      "Current Loss : 1825.2247103258655\n",
      "1279\n",
      "Current Loss : 1825.2158473809352\n",
      "1279\n",
      "Current Loss : 1825.1982296765816\n",
      "1279\n",
      "Current Loss : 1825.1529839924383\n",
      "1279\n",
      "Current Loss : 1825.1087545728433\n",
      "1279\n",
      "Current Loss : 1825.0914507070324\n",
      "1279\n",
      "Current Loss : 1825.0474694640363\n",
      "1279\n",
      "Current Loss : 1825.0305582285425\n",
      "1279\n",
      "Current Loss : 1824.987632694393\n",
      "1279\n",
      "Current Loss : 1824.9456489743588\n",
      "1279\n",
      "Current Loss : 1824.9369339905406\n",
      "1279\n",
      "Current Loss : 1824.9317782656756\n",
      "1279\n",
      "Current Loss : 1824.8893756527743\n",
      "1279\n",
      "Current Loss : 1824.8808392724625\n",
      "1279\n",
      "Current Loss : 1824.8397951624986\n",
      "1279\n",
      "Current Loss : 1824.8348504319667\n",
      "1279\n",
      "Current Loss : 1824.8300204276989\n",
      "1279\n",
      "Current Loss : 1824.790354715877\n",
      "1279\n",
      "Current Loss : 1824.785689532673\n",
      "1279\n",
      "Current Loss : 1824.7773078590126\n",
      "1279\n",
      "Current Loss : 1824.7364508151852\n",
      "1279\n",
      "Current Loss : 1824.6964957919856\n",
      "1279\n",
      "Current Loss : 1824.6918373805609\n",
      "1279\n",
      "Current Loss : 1824.6755375473756\n",
      "1279\n",
      "Current Loss : 1824.6532676260767\n",
      "1279\n",
      "Current Loss : 1824.6152833633357\n",
      "1279\n",
      "Current Loss : 1824.5991815508482\n",
      "1279\n",
      "Current Loss : 1824.5909466849018\n",
      "1279\n",
      "Current Loss : 1824.5521859462524\n",
      "1279\n",
      "Current Loss : 1824.5142781978252\n",
      "1279\n",
      "Current Loss : 1824.4772020207308\n",
      "1279\n",
      "Current Loss : 1824.4407980792214\n",
      "1279\n",
      "Current Loss : 1824.4326081147576\n",
      "1279\n",
      "Current Loss : 1824.4280206251237\n",
      "1279\n",
      "Current Loss : 1824.391823447188\n",
      "1279\n",
      "Current Loss : 1824.3763444049978\n",
      "1279\n",
      "Current Loss : 1824.361135622131\n",
      "1279\n",
      "Current Loss : 1824.3530966921046\n",
      "1279\n",
      "Current Loss : 1824.3485856159957\n",
      "1279\n",
      "Current Loss : 1824.3134706822984\n",
      "1279\n",
      "Current Loss : 1824.309105365319\n",
      "1279\n",
      "Current Loss : 1824.294159709877\n",
      "1279\n",
      "Current Loss : 1824.2732585445701\n",
      "1279\n",
      "Current Loss : 1824.268988619241\n",
      "1279\n",
      "Current Loss : 1824.2489302345302\n",
      "1279\n",
      "Current Loss : 1824.2447365927758\n",
      "1279\n",
      "Current Loss : 1824.2406308400641\n",
      "1279\n",
      "Current Loss : 1824.2068468878758\n",
      "1279\n",
      "Current Loss : 1824.1718970801344\n",
      "1279\n",
      "Current Loss : 1824.1678914995525\n",
      "1279\n",
      "Current Loss : 1824.1639672258857\n",
      "1279\n",
      "Current Loss : 1824.1493522382887\n",
      "1279\n",
      "Current Loss : 1824.1151867380463\n",
      "1279\n",
      "Current Loss : 1824.081770526077\n",
      "1279\n",
      "Current Loss : 1824.0675543667567\n",
      "1279\n",
      "Current Loss : 1824.0636834538482\n",
      "1279\n",
      "Current Loss : 1824.0556217092349\n",
      "1279\n",
      "Current Loss : 1824.0518009150192\n",
      "1279\n",
      "Current Loss : 1824.0190844870895\n",
      "1279\n",
      "Current Loss : 1824.0052539306878\n",
      "1279\n",
      "Current Loss : 1823.973049598109\n",
      "1279\n",
      "Current Loss : 1823.941252993502\n",
      "1279\n",
      "Current Loss : 1823.91016171336\n",
      "1279\n",
      "Current Loss : 1823.8791902478804\n",
      "1279\n",
      "Current Loss : 1823.8712321448982\n",
      "1279\n",
      "Current Loss : 1823.8634214521765\n",
      "1279\n",
      "Current Loss : 1823.8499030511155\n",
      "1279\n",
      "Current Loss : 1823.8198884220828\n",
      "1279\n",
      "Current Loss : 1823.806535154958\n",
      "1279\n",
      "Current Loss : 1823.7989710659847\n",
      "1279\n",
      "Current Loss : 1823.7858798159734\n",
      "1279\n",
      "Current Loss : 1823.7730106183126\n",
      "1279\n",
      "Current Loss : 1823.760358804739\n",
      "1279\n",
      "Current Loss : 1823.7530305659654\n",
      "1279\n",
      "Current Loss : 1823.7493169674783\n",
      "1279\n",
      "Current Loss : 1823.7192799319953\n",
      "1279\n",
      "Current Loss : 1823.7120148249012\n",
      "1279\n",
      "Current Loss : 1823.6933637321804\n",
      "1279\n",
      "Current Loss : 1823.6810623309473\n",
      "1279\n",
      "Current Loss : 1823.6739633420148\n",
      "1279\n",
      "Current Loss : 1823.6560635748822\n",
      "1279\n",
      "Current Loss : 1823.6490943717106\n",
      "1279\n",
      "Current Loss : 1823.6453510750184\n",
      "1279\n",
      "Current Loss : 1823.6416847356604\n",
      "1279\n",
      "Current Loss : 1823.6127820861013\n",
      "1279\n",
      "Current Loss : 1823.6059212112777\n",
      "1279\n",
      "Current Loss : 1823.5779203478671\n",
      "1279\n",
      "Current Loss : 1823.5507568215437\n",
      "1279\n",
      "Current Loss : 1823.5440790935454\n",
      "1279\n",
      "Current Loss : 1823.5375177717497\n",
      "1279\n",
      "Current Loss : 1823.5339676866013\n",
      "1279\n",
      "Current Loss : 1823.5274983380043\n",
      "1279\n",
      "Current Loss : 1823.5211409509345\n",
      "1279\n",
      "Current Loss : 1823.5176186828526\n",
      "1279\n",
      "Current Loss : 1823.500597781245\n",
      "1279\n",
      "Current Loss : 1823.4943346333994\n",
      "1279\n",
      "Current Loss : 1823.4650964053367\n",
      "1279\n",
      "Current Loss : 1823.4365054062735\n",
      "1279\n",
      "Current Loss : 1823.4106474492064\n",
      "1279\n",
      "Current Loss : 1823.3855585749566\n",
      "1279\n",
      "Current Loss : 1823.3820916181173\n",
      "1279\n",
      "Current Loss : 1823.365961997724\n",
      "1279\n",
      "Current Loss : 1823.3625555014455\n",
      "1279\n",
      "Current Loss : 1823.3563448676923\n",
      "1279\n",
      "Current Loss : 1823.328576596394\n",
      "1279\n",
      "Current Loss : 1823.30142154114\n",
      "1279\n",
      "Current Loss : 1823.286035158713\n",
      "1279\n",
      "Current Loss : 1823.2712498471947\n",
      "1279\n",
      "Current Loss : 1823.259426871661\n",
      "1279\n",
      "Current Loss : 1823.245203259227\n",
      "1279\n",
      "Current Loss : 1823.2390644350314\n",
      "1279\n",
      "Current Loss : 1823.2356228708686\n",
      "1279\n",
      "Current Loss : 1823.2092212704788\n",
      "1279\n",
      "Current Loss : 1823.2031361435343\n",
      "1279\n",
      "Current Loss : 1823.1772764512432\n",
      "1279\n",
      "Current Loss : 1823.1536784910043\n",
      "1279\n",
      "Current Loss : 1823.128491051255\n",
      "1279\n",
      "Current Loss : 1823.103854110446\n",
      "1279\n",
      "Current Loss : 1823.1003928025182\n",
      "1279\n",
      "Current Loss : 1823.0943174565803\n",
      "1279\n",
      "Current Loss : 1823.0909019380588\n",
      "1279\n",
      "Current Loss : 1823.077442165098\n",
      "1279\n",
      "Current Loss : 1823.0661129938198\n",
      "1279\n",
      "Current Loss : 1823.062770927925\n",
      "1279\n",
      "Current Loss : 1823.0567914297135\n",
      "1279\n",
      "Current Loss : 1823.0438316576974\n",
      "1279\n",
      "Current Loss : 1823.0379612068728\n",
      "1279\n",
      "Current Loss : 1823.02688525603\n",
      "1279\n",
      "Current Loss : 1823.0211488895975\n",
      "1279\n",
      "Current Loss : 1823.0155153332075\n",
      "1279\n",
      "Current Loss : 1823.0099817428584\n",
      "1279\n",
      "Current Loss : 1823.006605914187\n",
      "1279\n",
      "Current Loss : 1822.9824113485474\n",
      "1279\n",
      "Current Loss : 1822.9587429994997\n",
      "1279\n",
      "Current Loss : 1822.9553857227222\n",
      "1279\n",
      "Current Loss : 1822.9447174661448\n",
      "1279\n",
      "Current Loss : 1822.9392009226983\n",
      "1279\n",
      "Current Loss : 1822.9267951911836\n",
      "1279\n",
      "Current Loss : 1822.9046326310538\n",
      "1279\n",
      "Current Loss : 1822.8816221608852\n",
      "1279\n",
      "Current Loss : 1822.8761989622328\n",
      "1279\n",
      "Current Loss : 1822.8548202578743\n",
      "1279\n",
      "Current Loss : 1822.849518267639\n",
      "1279\n",
      "Current Loss : 1822.8270434208605\n",
      "1279\n",
      "Current Loss : 1822.8237124374427\n",
      "1279\n",
      "Current Loss : 1822.813316863378\n",
      "1279\n",
      "Current Loss : 1822.7926738046845\n",
      "1279\n",
      "Current Loss : 1822.7809237663319\n",
      "1279\n",
      "Current Loss : 1822.759105058525\n",
      "1279\n",
      "Current Loss : 1822.7391979962636\n",
      "1279\n",
      "Current Loss : 1822.7179475579705\n",
      "1279\n",
      "Current Loss : 1822.698714543306\n",
      "1279\n",
      "Current Loss : 1822.6934936343325\n",
      "1279\n",
      "Current Loss : 1822.6823355506199\n",
      "1279\n",
      "Current Loss : 1822.6790938024076\n",
      "1279\n",
      "Current Loss : 1822.6739488829382\n",
      "1279\n",
      "Current Loss : 1822.6637377526797\n",
      "1279\n",
      "Current Loss : 1822.653698399816\n",
      "1279\n",
      "Current Loss : 1822.6486936591161\n",
      "1279\n",
      "Current Loss : 1822.6300603848872\n",
      "1279\n",
      "Current Loss : 1822.6201683616412\n",
      "1279\n",
      "Current Loss : 1822.6020310001632\n",
      "1279\n",
      "Current Loss : 1822.5913202433562\n",
      "1279\n",
      "Current Loss : 1822.5737324492275\n",
      "1279\n",
      "Current Loss : 1822.563440526574\n",
      "1279\n",
      "Current Loss : 1822.5431620897032\n",
      "1279\n",
      "Current Loss : 1822.5261856120753\n",
      "1279\n",
      "Current Loss : 1822.5163296788617\n",
      "1279\n",
      "Current Loss : 1822.5115071914931\n",
      "1279\n",
      "Current Loss : 1822.5020067906985\n",
      "1279\n",
      "Current Loss : 1822.4855803018354\n",
      "1279\n",
      "Current Loss : 1822.4757173591827\n",
      "1279\n",
      "Current Loss : 1822.4597124098245\n",
      "1279\n",
      "Current Loss : 1822.4402247472021\n",
      "1279\n",
      "Current Loss : 1822.431131779068\n",
      "1279\n",
      "Current Loss : 1822.4156741159877\n",
      "1279\n",
      "Current Loss : 1822.412607080064\n",
      "1279\n",
      "Current Loss : 1822.4096038615342\n",
      "1279\n",
      "Current Loss : 1822.3946250951583\n",
      "1279\n",
      "Current Loss : 1822.389937220338\n",
      "1279\n",
      "Current Loss : 1822.3710100724452\n",
      "1279\n",
      "Current Loss : 1822.368051236738\n",
      "1279\n",
      "Current Loss : 1822.363404869309\n",
      "1279\n",
      "Current Loss : 1822.3547009037509\n",
      "1279\n",
      "Current Loss : 1822.3463004114228\n",
      "1279\n",
      "Current Loss : 1822.3319106557392\n",
      "1279\n",
      "Current Loss : 1822.3273727776254\n",
      "1279\n",
      "Current Loss : 1822.3089426490942\n",
      "1279\n",
      "Current Loss : 1822.2950459354454\n",
      "1279\n",
      "Current Loss : 1822.2815335957152\n",
      "1279\n",
      "Current Loss : 1822.278638635915\n",
      "1279\n",
      "Current Loss : 1822.2607437141298\n",
      "1279\n",
      "Current Loss : 1822.2476954440344\n",
      "1279\n",
      "Current Loss : 1822.2379914980988\n",
      "1279\n",
      "Current Loss : 1822.230001817399\n",
      "1279\n",
      "Current Loss : 1822.2126533707594\n",
      "1279\n",
      "Current Loss : 1822.2049691498514\n",
      "1279\n",
      "Current Loss : 1822.2021228617198\n",
      "1279\n",
      "Current Loss : 1822.1926201870622\n",
      "1279\n",
      "Current Loss : 1822.1881625644285\n",
      "1279\n",
      "Current Loss : 1822.1755323127782\n",
      "1279\n",
      "Current Loss : 1822.1632472201663\n",
      "1279\n",
      "Current Loss : 1822.1604896562988\n",
      "1279\n",
      "Current Loss : 1822.1561338449792\n",
      "1279\n",
      "Current Loss : 1822.1467705681048\n",
      "1279\n",
      "Current Loss : 1822.1375629217116\n",
      "1279\n",
      "Current Loss : 1822.1285075417516\n",
      "1279\n",
      "Current Loss : 1822.121074233342\n",
      "1279\n",
      "Current Loss : 1822.1044027883468\n",
      "1279\n",
      "Current Loss : 1822.1001700166273\n",
      "1279\n",
      "Current Loss : 1822.0913186200203\n",
      "1279\n",
      "Current Loss : 1822.0886061915521\n",
      "1279\n",
      "Current Loss : 1822.0723068616926\n",
      "1279\n",
      "Current Loss : 1822.0563681396598\n",
      "1279\n",
      "Current Loss : 1822.04455686387\n",
      "1279\n",
      "Current Loss : 1822.0374711351562\n",
      "1279\n",
      "Current Loss : 1822.034793796241\n",
      "1279\n",
      "Current Loss : 1822.02794228649\n",
      "1279\n",
      "Current Loss : 1822.021315701254\n",
      "1279\n",
      "Current Loss : 1822.0186873144273\n",
      "1279\n",
      "Current Loss : 1822.0032207062222\n",
      "1279\n",
      "Current Loss : 1822.000623605679\n",
      "1279\n",
      "Current Loss : 1821.9920241666548\n",
      "1279\n",
      "Current Loss : 1821.9894843695133\n",
      "1279\n",
      "Current Loss : 1821.9810370706325\n",
      "1279\n",
      "Current Loss : 1821.974637585193\n",
      "1279\n",
      "Current Loss : 1821.963259058062\n",
      "1279\n",
      "Current Loss : 1821.9482720751696\n",
      "1279\n",
      "Current Loss : 1821.9372644279774\n",
      "1279\n",
      "Current Loss : 1821.928920513138\n",
      "1279\n",
      "Current Loss : 1821.9207132300185\n",
      "1279\n",
      "Current Loss : 1821.9099373522677\n",
      "1279\n",
      "Current Loss : 1821.9075105405127\n",
      "1279\n",
      "Current Loss : 1821.8994150828266\n",
      "1279\n",
      "Current Loss : 1821.8849813245658\n",
      "1279\n",
      "Current Loss : 1821.8788440420863\n",
      "1279\n",
      "Current Loss : 1821.876455193444\n",
      "1279\n",
      "Current Loss : 1821.8741084071276\n",
      "1279\n",
      "Current Loss : 1821.8699164001255\n",
      "1279\n",
      "Current Loss : 1821.8675952360554\n",
      "1279\n",
      "Current Loss : 1821.863472189801\n",
      "1279\n",
      "Current Loss : 1821.859428220574\n",
      "1279\n",
      "Current Loss : 1821.8490937008294\n",
      "1279\n",
      "Current Loss : 1821.8468000865093\n",
      "1279\n",
      "Current Loss : 1821.8326478763547\n",
      "1279\n",
      "Current Loss : 1821.828660158617\n",
      "1279\n",
      "Current Loss : 1821.8263771858258\n",
      "1279\n",
      "Current Loss : 1821.8241338062844\n",
      "1279\n",
      "Current Loss : 1821.810242031949\n",
      "1279\n",
      "Current Loss : 1821.8062787757838\n",
      "1279\n",
      "Current Loss : 1821.7985429716275\n",
      "1279\n",
      "Current Loss : 1821.7926661575345\n",
      "1279\n",
      "Current Loss : 1821.785049235983\n",
      "1279\n",
      "Current Loss : 1821.7828349387885\n",
      "1279\n",
      "Current Loss : 1821.7693111517333\n",
      "1279\n",
      "Current Loss : 1821.7671200091634\n",
      "1279\n",
      "Current Loss : 1821.764966243044\n",
      "1279\n",
      "Current Loss : 1821.7517090233302\n",
      "1279\n",
      "Current Loss : 1821.7477763935658\n",
      "1279\n",
      "Current Loss : 1821.7404016936912\n",
      "1279\n",
      "Current Loss : 1821.7306369166383\n",
      "1279\n",
      "Current Loss : 1821.7233539166336\n",
      "1279\n",
      "Current Loss : 1821.719545342519\n",
      "1279\n",
      "Current Loss : 1821.7158057326171\n",
      "1279\n",
      "Current Loss : 1821.7086770609935\n",
      "1279\n",
      "Current Loss : 1821.7065481909679\n",
      "1279\n",
      "Current Loss : 1821.6970350503532\n",
      "1279\n",
      "Current Loss : 1821.6933820187237\n",
      "1279\n",
      "Current Loss : 1821.6897940281085\n",
      "1279\n",
      "Current Loss : 1821.6805643721293\n",
      "1279\n",
      "Current Loss : 1821.678476622585\n",
      "1279\n",
      "Current Loss : 1821.6764246156688\n",
      "1279\n",
      "Current Loss : 1821.6635849621805\n",
      "1279\n",
      "Current Loss : 1821.6510284953422\n",
      "1279\n",
      "Current Loss : 1821.6474552045652\n",
      "1279\n",
      "Current Loss : 1821.6405317664958\n",
      "1279\n",
      "Current Loss : 1821.6337211551293\n",
      "1279\n",
      "Current Loss : 1821.627021266818\n",
      "1279\n",
      "Current Loss : 1821.6235609787686\n",
      "1279\n",
      "Current Loss : 1821.6215175313914\n",
      "1279\n",
      "Current Loss : 1821.6195086234143\n",
      "1279\n",
      "Current Loss : 1821.6072598937355\n",
      "1279\n",
      "Current Loss : 1821.605270715116\n",
      "1279\n",
      "Current Loss : 1821.598753061481\n",
      "1279\n",
      "Current Loss : 1821.5953165150104\n",
      "1279\n",
      "Current Loss : 1821.5864957432332\n",
      "1279\n",
      "Current Loss : 1821.5831327674134\n",
      "1279\n",
      "Current Loss : 1821.5745614521365\n",
      "1279\n",
      "Current Loss : 1821.5662184661885\n",
      "1279\n",
      "Current Loss : 1821.5606717015296\n",
      "1279\n",
      "Current Loss : 1821.5542077407392\n",
      "1279\n",
      "Current Loss : 1821.5424156384483\n",
      "1279\n",
      "Current Loss : 1821.5360872992487\n",
      "1279\n",
      "Current Loss : 1821.5341607406278\n",
      "1279\n",
      "Current Loss : 1821.5308886584085\n",
      "1279\n",
      "Current Loss : 1821.5276739026415\n",
      "1279\n",
      "Current Loss : 1821.5195936625405\n",
      "1279\n",
      "Current Loss : 1821.5142521360635\n",
      "1279\n",
      "Current Loss : 1821.508033287016\n",
      "1279\n",
      "Current Loss : 1821.5028560515586\n",
      "1279\n",
      "Current Loss : 1821.4978421538708\n",
      "1279\n",
      "Current Loss : 1821.4917098512476\n",
      "1279\n",
      "Current Loss : 1821.4856768135026\n",
      "1279\n",
      "Current Loss : 1821.4825873189186\n",
      "1279\n",
      "Current Loss : 1821.4806997871879\n",
      "1279\n",
      "Current Loss : 1821.4776546010078\n",
      "1279\n",
      "Current Loss : 1821.4727909174226\n",
      "1279\n",
      "Current Loss : 1821.4680775847742\n",
      "1279\n",
      "Current Loss : 1821.4635081750378\n",
      "1279\n",
      "Current Loss : 1821.452184822474\n",
      "1279\n",
      "Current Loss : 1821.44918769897\n",
      "1279\n",
      "Current Loss : 1821.4413686807031\n",
      "1279\n",
      "Current Loss : 1821.4369642119161\n",
      "1279\n",
      "Current Loss : 1821.425943576935\n",
      "1279\n",
      "Current Loss : 1821.4200650329788\n",
      "1279\n",
      "Current Loss : 1821.4142817513516\n",
      "1279\n",
      "Current Loss : 1821.4124119030594\n",
      "1279\n",
      "Current Loss : 1821.4081468030101\n",
      "1279\n",
      "Current Loss : 1821.4024573042109\n",
      "1279\n",
      "Current Loss : 1821.3948385826254\n",
      "1279\n",
      "Current Loss : 1821.3874173630768\n",
      "1279\n",
      "Current Loss : 1821.3801875124516\n",
      "1279\n",
      "Current Loss : 1821.3731432107882\n",
      "1279\n",
      "Current Loss : 1821.366278896293\n",
      "1279\n",
      "Current Loss : 1821.3621403322165\n",
      "1279\n",
      "Current Loss : 1821.351705109952\n",
      "1279\n",
      "Current Loss : 1821.3488405778558\n",
      "1279\n",
      "Current Loss : 1821.342206222273\n",
      "1279\n",
      "Current Loss : 1821.3394049636904\n",
      "1279\n",
      "Current Loss : 1821.333721929214\n",
      "1279\n",
      "Current Loss : 1821.3272453292293\n",
      "1279\n",
      "Current Loss : 1821.3232476761962\n",
      "1279\n",
      "Current Loss : 1821.3205243585376\n",
      "1279\n",
      "Current Loss : 1821.3178498012614\n",
      "1279\n",
      "Current Loss : 1821.3122569622465\n",
      "1279\n",
      "Current Loss : 1821.3096428177817\n",
      "1279\n",
      "Current Loss : 1821.3033398801097\n",
      "1279\n",
      "Current Loss : 1821.2978295302096\n",
      "1279\n",
      "Current Loss : 1821.2924081523513\n",
      "1279\n",
      "Current Loss : 1821.2885161638685\n",
      "1279\n",
      "Current Loss : 1821.2831755026953\n",
      "1279\n",
      "Current Loss : 1821.2806590911487\n",
      "1279\n",
      "Current Loss : 1821.2744627222826\n",
      "1279\n",
      "Current Loss : 1821.272000483999\n",
      "1279\n",
      "Current Loss : 1821.269581524861\n",
      "1279\n",
      "Current Loss : 1821.2635587147681\n",
      "1279\n",
      "Current Loss : 1821.2536241901648\n",
      "1279\n",
      "Current Loss : 1821.2512431310838\n",
      "1279\n",
      "Current Loss : 1821.2415200599114\n",
      "1279\n",
      "Current Loss : 1821.2377860486527\n",
      "1279\n",
      "Current Loss : 1821.235987013849\n",
      "1279\n",
      "Current Loss : 1821.2264895427502\n",
      "1279\n",
      "Current Loss : 1821.2172030145298\n",
      "1279\n",
      "Current Loss : 1821.2154197871112\n",
      "1279\n",
      "Current Loss : 1821.2136706521899\n",
      "1279\n",
      "Current Loss : 1821.2100823388892\n",
      "1279\n",
      "Current Loss : 1821.2009970361771\n",
      "1279\n",
      "Current Loss : 1821.1975272463574\n",
      "1279\n",
      "Current Loss : 1821.18865895356\n",
      "1279\n",
      "Current Loss : 1821.185302773527\n",
      "1279\n",
      "Current Loss : 1821.1766456133666\n",
      "1279\n",
      "Current Loss : 1821.1681779492199\n",
      "1279\n",
      "Current Loss : 1821.1664223476143\n",
      "1279\n",
      "Current Loss : 1821.1581291295086\n",
      "1279\n",
      "Current Loss : 1821.1556610433383\n",
      "1279\n",
      "Current Loss : 1821.1524510760162\n",
      "1279\n",
      "Current Loss : 1821.1469852997584\n",
      "1279\n",
      "Current Loss : 1821.1389123525796\n",
      "1279\n",
      "Current Loss : 1821.1364885275934\n",
      "1279\n",
      "Current Loss : 1821.134108116266\n",
      "1279\n",
      "Current Loss : 1821.132346417569\n",
      "1279\n",
      "Current Loss : 1821.1270837662432\n",
      "1279\n",
      "Current Loss : 1821.1191865839337\n",
      "1279\n",
      "Current Loss : 1821.1142068799934\n",
      "1279\n",
      "Current Loss : 1821.1065046024244\n",
      "1279\n",
      "Current Loss : 1821.1014276233504\n",
      "1279\n",
      "Current Loss : 1821.0964794485396\n",
      "1279\n",
      "Current Loss : 1821.0916561240026\n",
      "1279\n",
      "Current Loss : 1821.089329478065\n",
      "1279\n",
      "Current Loss : 1821.0846354998093\n",
      "1279\n",
      "Current Loss : 1821.0815640466656\n",
      "1279\n",
      "Current Loss : 1821.076624305101\n",
      "1279\n",
      "Current Loss : 1821.0692475040637\n",
      "1279\n",
      "Current Loss : 1821.0669733717298\n",
      "1279\n",
      "Current Loss : 1821.0597469174272\n",
      "1279\n",
      "Current Loss : 1821.0526789058947\n",
      "1279\n",
      "Current Loss : 1821.0457654836125\n",
      "1279\n",
      "Current Loss : 1821.0434955244668\n",
      "1279\n",
      "Current Loss : 1821.0367211802818\n",
      "1279\n",
      "Current Loss : 1821.0319893031517\n",
      "1279\n",
      "Current Loss : 1821.0273342533762\n",
      "1279\n",
      "Current Loss : 1821.025114477557\n",
      "1279\n",
      "Current Loss : 1821.0205456608217\n",
      "1279\n",
      "Current Loss : 1821.0160831484145\n",
      "1279\n",
      "Current Loss : 1821.0095350196373\n",
      "1279\n",
      "Current Loss : 1821.0066211457242\n",
      "1279\n",
      "Current Loss : 1821.002300196026\n",
      "1279\n",
      "Current Loss : 1821.000556185416\n",
      "1279\n",
      "Current Loss : 1820.9988436339606\n",
      "1279\n",
      "Current Loss : 1820.992462462411\n",
      "1279\n",
      "Current Loss : 1820.990770481299\n",
      "1279\n",
      "Current Loss : 1820.9845194708835\n",
      "1279\n",
      "Current Loss : 1820.9828470255202\n",
      "1279\n",
      "Current Loss : 1820.9812024874363\n",
      "1279\n",
      "Current Loss : 1820.976759633013\n",
      "1279\n",
      "Current Loss : 1820.9726312820248\n",
      "1279\n",
      "Current Loss : 1820.9665451328292\n",
      "1279\n",
      "Current Loss : 1820.9605917497502\n",
      "1279\n",
      "Current Loss : 1820.954768040616\n",
      "1279\n",
      "Current Loss : 1820.9531382536647\n",
      "1279\n",
      "Current Loss : 1820.9492033953097\n",
      "1279\n",
      "Current Loss : 1820.9476081925684\n",
      "1279\n",
      "Current Loss : 1820.9432751693907\n",
      "1279\n",
      "Current Loss : 1820.9410359024835\n",
      "1279\n",
      "Current Loss : 1820.9394646265507\n",
      "1279\n",
      "Current Loss : 1820.935218225776\n",
      "1279\n",
      "Current Loss : 1820.9330212799543\n",
      "1279\n",
      "Current Loss : 1820.9308630380472\n",
      "1279\n",
      "Current Loss : 1820.9267058565529\n",
      "1279\n",
      "Current Loss : 1820.9228649785066\n",
      "1279\n",
      "Current Loss : 1820.9191210565289\n",
      "1279\n",
      "Current Loss : 1820.9134967054695\n",
      "1279\n",
      "Current Loss : 1820.9079955139584\n",
      "1279\n",
      "Current Loss : 1820.90587788935\n",
      "1279\n",
      "Current Loss : 1820.904319098916\n",
      "1279\n",
      "Current Loss : 1820.902785737351\n",
      "1279\n",
      "Current Loss : 1820.8987237440585\n",
      "1279\n",
      "Current Loss : 1820.8966377702122\n",
      "1279\n",
      "Current Loss : 1820.8926512157575\n",
      "1279\n",
      "Current Loss : 1820.889052046322\n",
      "1279\n",
      "Current Loss : 1820.887018392439\n",
      "1279\n",
      "Current Loss : 1820.8830924442505\n",
      "1279\n",
      "Current Loss : 1820.8815933918281\n",
      "1279\n",
      "Current Loss : 1820.8788560394999\n",
      "1279\n",
      "Current Loss : 1820.8749940224875\n",
      "1279\n",
      "Current Loss : 1820.8696604681274\n",
      "1279\n",
      "Current Loss : 1820.8676681206537\n",
      "1279\n",
      "Current Loss : 1820.8641765188074\n",
      "1279\n",
      "Current Loss : 1820.8627012021238\n",
      "1279\n",
      "Current Loss : 1820.8593042220616\n",
      "1279\n",
      "Current Loss : 1820.8555097580509\n",
      "1279\n",
      "Current Loss : 1820.8517764234819\n",
      "1279\n",
      "Current Loss : 1820.8484391565507\n",
      "1279\n",
      "Current Loss : 1820.8447534040097\n",
      "1279\n",
      "Current Loss : 1820.8414867297554\n",
      "1279\n",
      "Current Loss : 1820.837847938041\n",
      "1279\n",
      "Current Loss : 1820.8351765786035\n",
      "1279\n",
      "Current Loss : 1820.832576782511\n",
      "1279\n",
      "Current Loss : 1820.830045597014\n",
      "1279\n",
      "Current Loss : 1820.8281541401768\n",
      "1279\n",
      "Current Loss : 1820.8256930045209\n",
      "1279\n",
      "Current Loss : 1820.8242845679194\n",
      "1279\n",
      "Current Loss : 1820.8192339570214\n",
      "1279\n",
      "Current Loss : 1820.8142948627963\n",
      "1279\n",
      "Current Loss : 1820.812895106983\n",
      "1279\n",
      "Current Loss : 1820.8110090075181\n",
      "1279\n",
      "Current Loss : 1820.8096253191936\n",
      "1279\n",
      "Current Loss : 1820.8060892142294\n",
      "1279\n",
      "Current Loss : 1820.8047330512768\n",
      "1279\n",
      "Current Loss : 1820.801600180743\n",
      "1279\n",
      "Current Loss : 1820.7992309397705\n",
      "1279\n",
      "Current Loss : 1820.7961739997586\n",
      "1279\n",
      "Current Loss : 1820.7931920830601\n",
      "1279\n",
      "Current Loss : 1820.7913554573774\n",
      "1279\n",
      "Current Loss : 1820.7890481361428\n",
      "1279\n",
      "Current Loss : 1820.7877303772736\n",
      "1279\n",
      "Current Loss : 1820.7854838987728\n",
      "1279\n",
      "Current Loss : 1820.7841898073232\n",
      "1279\n",
      "Current Loss : 1820.7829166884262\n",
      "1279\n",
      "Current Loss : 1820.7781365086114\n",
      "1279\n",
      "Current Loss : 1820.7752689588206\n",
      "1279\n",
      "Current Loss : 1820.7717723218384\n",
      "1279\n",
      "Current Loss : 1820.7689634670355\n",
      "1279\n",
      "Current Loss : 1820.7643444594837\n",
      "1279\n",
      "Current Loss : 1820.760907404332\n",
      "1279\n",
      "Current Loss : 1820.7581759135792\n",
      "1279\n",
      "Current Loss : 1820.7563759751592\n",
      "1279\n",
      "Current Loss : 1820.7542004237137\n",
      "1279\n",
      "Current Loss : 1820.7529567082154\n",
      "1279\n",
      "Current Loss : 1820.7517331383622\n",
      "1279\n",
      "Current Loss : 1820.7499565664127\n",
      "1279\n",
      "Current Loss : 1820.7487464135686\n",
      "1279\n",
      "Current Loss : 1820.7442367886358\n",
      "1279\n",
      "Current Loss : 1820.742135503148\n"
     ]
    }
   ],
   "source": [
    "m = A_train.shape[0] # set m to training examples , need to put m as param into function\n",
    "#gradient_descent(X,A_train, label_train,lr = 0.0025, iterations=100)\n",
    "BCGD_randomized(X,A_train, label_train, iterations=1000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
